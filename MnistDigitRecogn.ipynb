{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BaBkn7yGy8NB"
   },
   "source": [
    "This is a MNIST digit recognition without a hidden layer. it was just the input and output layers with softmax activation function, cross entropy as loss function, batch size of 100 and in the range of 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIq8gtw_MLPT"
   },
   "source": [
    "#**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1788,
     "status": "ok",
     "timestamp": 1540004854948,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "802xR-mIJHHI",
    "outputId": "0008409d-3e0d-4bb8-d050-45f6c640cf95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/DATA/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from  tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST/DATA', one_hot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvAXYMm7_BoO"
   },
   "outputs": [],
   "source": [
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "x_val = mnist.validation.images\n",
    "y_val = mnist.validation.labels\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdrAWZJYMbEb"
   },
   "source": [
    "the shapes of the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1386,
     "status": "ok",
     "timestamp": 1539949452669,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "sej-5WSZLKcB",
    "outputId": "00d7f0fc-2ee2-4c26-a976-9014d3170611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training set is: (55000, 784), this means that it has 55000 examples\n",
      "The shape of the validation set is: (5000, 784), this means that it has 5000 examples\n",
      "The shape of the test set is: (10000, 784), this means that it has 10000 examples\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the training set is: {0}, this means that it has {1} examples'.format(x_train.shape, y_train.shape[0]))\n",
    "print('The shape of the validation set is: {0}, this means that it has {1} examples'.format(x_val.shape, y_val.shape[0]))\n",
    "print('The shape of the test set is: {0}, this means that it has {1} examples'.format(x_test.shape, y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2346
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1371,
     "status": "ok",
     "timestamp": 1539949852998,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "6NbC95iEIZaR",
    "outputId": "1e51093a-2a70-45f9-9109-eea18a0b6bb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.14901961 0.18823531\n",
      " 0.18823531 0.08627451 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.24313727\n",
      " 0.3803922  0.77647066 0.95294124 0.9960785  0.9960785  0.8313726\n",
      " 0.10588236 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.2627451  0.6745098  0.9960785  0.9960785\n",
      " 0.882353   0.854902   0.854902   0.9294118  0.9725491  0.15686275\n",
      " 0.         0.08235294 0.6431373  0.73333335 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.34901962\n",
      " 0.8588236  0.9960785  0.3803922  0.2627451  0.05490196 0.\n",
      " 0.         0.36078432 0.9058824  0.4784314  0.09019608 0.7960785\n",
      " 0.92549026 0.23137257 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09803922 0.85098046 0.9490197  0.36078432\n",
      " 0.01568628 0.         0.         0.         0.         0.01568628\n",
      " 0.5764706  0.9921569  0.94117653 0.909804   0.36078432 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.39607847 1.         0.36078432 0.         0.         0.\n",
      " 0.         0.         0.         0.41176474 0.9960785  0.9960785\n",
      " 0.69411767 0.04313726 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.654902   0.9568628\n",
      " 0.16078432 0.         0.         0.         0.02745098 0.29803923\n",
      " 0.7803922  0.9333334  0.93725497 0.36862746 0.03921569 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.75294125 0.47450984 0.         0.\n",
      " 0.00784314 0.24705884 0.7058824  0.9960785  0.91372555 0.49411768\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.74509805 0.7686275  0.05490196 0.00784314 0.3803922  0.9960785\n",
      " 0.98823535 0.57254905 0.20392159 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.50980395 0.882353\n",
      " 0.2784314  0.7058824  0.909804   0.70980394 0.23529413 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.50980395 0.9960785  0.9960785  0.90196085\n",
      " 0.18039216 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.02352941 0.3019608\n",
      " 0.9568628  0.9960785  0.63529414 0.01568628 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.43137258 0.9960785  0.854902   0.9960785\n",
      " 0.454902   0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.5137255\n",
      " 0.9960785  0.6039216  0.10980393 0.8352942  0.3372549  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.25882354 0.8196079  0.6        0.07450981\n",
      " 0.07450981 0.91372555 0.23529413 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.5568628  0.9960785  0.64705884 0.         0.05490196 0.8470589\n",
      " 0.654902   0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.3529412  0.9960785\n",
      " 0.6862745  0.         0.07058824 0.8980393  0.36078432 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.10196079 0.8980393  0.97647065 0.6901961\n",
      " 0.8705883  0.9568628  0.17254902 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.28627452 0.7568628  0.7725491  0.5254902  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "#overview of the values in a pixel(28 by 28)\n",
    "print(x_train[54999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oi2bGp5KPR0S"
   },
   "source": [
    "# Build a softmax regression model(1 layer neural network) and train on the dataset\n",
    "\n",
    "Build a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2684
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2058,
     "status": "ok",
     "timestamp": 1539948122505,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "SxIyiaCHzBee",
    "outputId": "1c02c3af-b827-48d9-8511-6d9e2f5e0c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:0.13train loss:235.23341||test acc:0.126test loss:23952.75\n",
      "\n",
      "\n",
      "train acc:0.26train loss:212.69957||test acc:0.1725test loss:22760.25\n",
      "\n",
      "\n",
      "train acc:0.22train loss:209.23715||test acc:0.2023test loss:21887.164\n",
      "\n",
      "\n",
      "train acc:0.3train loss:202.30441||test acc:0.2594test loss:20976.738\n",
      "\n",
      "\n",
      "train acc:0.33train loss:202.17181||test acc:0.3165test loss:20022.256\n",
      "\n",
      "\n",
      "train acc:0.45train loss:181.056||test acc:0.3759test loss:19237.29\n",
      "\n",
      "\n",
      "train acc:0.51train loss:173.40645||test acc:0.4158test loss:18553.572\n",
      "\n",
      "\n",
      "train acc:0.37train loss:180.3234||test acc:0.4433test loss:17859.762\n",
      "\n",
      "\n",
      "train acc:0.4train loss:175.7066||test acc:0.4757test loss:17135.873\n",
      "\n",
      "\n",
      "train acc:0.52train loss:164.38759||test acc:0.5119test loss:16576.838\n",
      "\n",
      "\n",
      "train acc:0.47train loss:167.70724||test acc:0.5398test loss:16102.065\n",
      "\n",
      "\n",
      "train acc:0.55train loss:151.06436||test acc:0.5462test loss:15770.166\n",
      "\n",
      "\n",
      "train acc:0.6train loss:140.7935||test acc:0.577test loss:15216.571\n",
      "\n",
      "\n",
      "train acc:0.65train loss:144.54523||test acc:0.6025test loss:14754.391\n",
      "\n",
      "\n",
      "train acc:0.63train loss:143.30875||test acc:0.612test loss:14416.256\n",
      "\n",
      "\n",
      "train acc:0.61train loss:134.63019||test acc:0.6292test loss:14005.923\n",
      "\n",
      "\n",
      "train acc:0.6train loss:132.04343||test acc:0.6341test loss:13663.913\n",
      "\n",
      "\n",
      "train acc:0.68train loss:134.72498||test acc:0.6511test loss:13314.8125\n",
      "\n",
      "\n",
      "train acc:0.67train loss:131.80066||test acc:0.674test loss:12914.232\n",
      "\n",
      "\n",
      "train acc:0.66train loss:128.30661||test acc:0.689test loss:12549.55\n",
      "\n",
      "\n",
      "train acc:0.72train loss:109.62398||test acc:0.6912test loss:12281.771\n",
      "\n",
      "\n",
      "train acc:0.73train loss:124.70424||test acc:0.7026test loss:11979.351\n",
      "\n",
      "\n",
      "train acc:0.79train loss:110.797134||test acc:0.708test loss:11774.471\n",
      "\n",
      "\n",
      "train acc:0.76train loss:115.62642||test acc:0.7032test loss:11618.692\n",
      "\n",
      "\n",
      "train acc:0.7train loss:118.4315||test acc:0.7209test loss:11277.691\n",
      "\n",
      "\n",
      "train acc:0.78train loss:107.1377||test acc:0.731test loss:11000.58\n",
      "\n",
      "\n",
      "train acc:0.73train loss:118.45975||test acc:0.7375test loss:10857.081\n",
      "\n",
      "\n",
      "train acc:0.73train loss:113.53786||test acc:0.7518test loss:10634.334\n",
      "\n",
      "\n",
      "train acc:0.88train loss:92.58801||test acc:0.7539test loss:10449.706\n",
      "\n",
      "\n",
      "train acc:0.79train loss:94.989365||test acc:0.7495test loss:10314.846\n",
      "\n",
      "\n",
      "train acc:0.71train loss:113.77765||test acc:0.7584test loss:10164.06\n",
      "\n",
      "\n",
      "train acc:0.77train loss:100.01773||test acc:0.7679test loss:10019.916\n",
      "\n",
      "\n",
      "train acc:0.78train loss:101.526764||test acc:0.7711test loss:9870.809\n",
      "\n",
      "\n",
      "train acc:0.82train loss:84.18077||test acc:0.7734test loss:9720.912\n",
      "\n",
      "\n",
      "train acc:0.72train loss:107.37696||test acc:0.7776test loss:9610.09\n",
      "\n",
      "\n",
      "train acc:0.71train loss:104.13296||test acc:0.7833test loss:9454.295\n",
      "\n",
      "\n",
      "train acc:0.8train loss:93.18098||test acc:0.7825test loss:9346.283\n",
      "\n",
      "\n",
      "train acc:0.85train loss:83.54926||test acc:0.788test loss:9212.573\n",
      "\n",
      "\n",
      "train acc:0.81train loss:91.58578||test acc:0.7826test loss:9141.961\n",
      "\n",
      "\n",
      "train acc:0.81train loss:83.29855||test acc:0.7876test loss:9013.024\n",
      "\n",
      "\n",
      "train acc:0.78train loss:89.829605||test acc:0.791test loss:8939.654\n",
      "\n",
      "\n",
      "train acc:0.81train loss:88.314896||test acc:0.795test loss:8824.482\n",
      "\n",
      "\n",
      "train acc:0.83train loss:90.35579||test acc:0.8002test loss:8689.069\n",
      "\n",
      "\n",
      "train acc:0.81train loss:79.3376||test acc:0.7979test loss:8595.924\n",
      "\n",
      "\n",
      "train acc:0.86train loss:80.314514||test acc:0.7974test loss:8522.304\n",
      "\n",
      "\n",
      "train acc:0.78train loss:93.33992||test acc:0.8022test loss:8437.132\n",
      "\n",
      "\n",
      "train acc:0.84train loss:77.07117||test acc:0.8014test loss:8381.531\n",
      "\n",
      "\n",
      "train acc:0.82train loss:81.01175||test acc:0.8008test loss:8319.092\n",
      "\n",
      "\n",
      "train acc:0.86train loss:74.22316||test acc:0.8021test loss:8266.811\n",
      "\n",
      "\n",
      "train acc:0.81train loss:87.53737||test acc:0.8112test loss:8125.7305\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inputs\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "#labels(one hot encoded)\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "#weights\n",
    "w = tf.Variable(tf.truncated_normal([784,10],stddev = 0.1), name = 'w')\n",
    "#biases\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "\n",
    "#weighted sum\n",
    "z = tf.matmul(tf.reshape(x,[-1,784]),w)+b\n",
    "\n",
    "#softmax activation\n",
    "h = tf.nn.softmax(z)\n",
    "\n",
    "#softmax cross entropy loss \n",
    "cross_entropy = -tf.reduce_sum(y*tf.log(h))\n",
    "\n",
    "#accuracy of the trained model between worst(0) and best(1)\n",
    "is_correct = tf.equal(tf.argmax(y,1),tf.argmax(h,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "#initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(50):\n",
    "  batch_x, batch_y = mnist.train.next_batch(100)\n",
    "  train_data = {x:batch_x, y:batch_y}\n",
    "  \n",
    "  sess.run(train_step, feed_dict=train_data)\n",
    "  \n",
    "  a_train, c_train = sess.run([accuracy,cross_entropy],feed_dict=train_data)\n",
    "  \n",
    "  test_data = {x:mnist.test.images, y:mnist.test.labels}\n",
    "  a_test, c_test = sess.run([accuracy,cross_entropy],feed_dict=test_data)\n",
    "  \n",
    "  print('train acc:' + str(a_train) + 'train loss:' + str(c_train) + '||' + 'test acc:' + str(a_test) + 'test loss:' + str(c_test))\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1025,
     "status": "ok",
     "timestamp": 1539948131938,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "pJxDiY0KDQ71",
    "outputId": "7c4f8330-14cc-43b2-ed0b-53f2a64e7bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is:  81.12%\n"
     ]
    }
   ],
   "source": [
    "a_test, c_test  = a_test,c_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "\n",
    "print('Test accuracy is: ', (\"%.2f\"%(a_test*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pepZivM0GUSp"
   },
   "source": [
    "# **With one sigmoid (hidden layer) layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5350
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8500,
     "status": "ok",
     "timestamp": 1539948186938,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "3IJGUcx01G3N",
    "outputId": "0f9502bb-5ee5-4bb0-e3b3-2cb0765203f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_train: 0.15loss_train: 1731.3469accuracy_test: 0.1079loss_test: 199604.88\n",
      "\n",
      "\n",
      "accuracy_train: 0.09loss_train: 291.48145accuracy_test: 0.1135loss_test: 28801.895\n",
      "\n",
      "\n",
      "accuracy_train: 0.2loss_train: 249.35802accuracy_test: 0.1276loss_test: 24478.34\n",
      "\n",
      "\n",
      "accuracy_train: 0.25loss_train: 241.768accuracy_test: 0.2019loss_test: 23158.715\n",
      "\n",
      "\n",
      "accuracy_train: 0.29loss_train: 219.24371accuracy_test: 0.1927loss_test: 22286.982\n",
      "\n",
      "\n",
      "accuracy_train: 0.22loss_train: 216.97064accuracy_test: 0.1986loss_test: 21483.398\n",
      "\n",
      "\n",
      "accuracy_train: 0.35loss_train: 201.65479accuracy_test: 0.2949loss_test: 20639.361\n",
      "\n",
      "\n",
      "accuracy_train: 0.46loss_train: 183.79816accuracy_test: 0.4033loss_test: 19740.41\n",
      "\n",
      "\n",
      "accuracy_train: 0.33loss_train: 203.96597accuracy_test: 0.3658loss_test: 19403.738\n",
      "\n",
      "\n",
      "accuracy_train: 0.54loss_train: 173.41359accuracy_test: 0.4897loss_test: 17607.07\n",
      "\n",
      "\n",
      "accuracy_train: 0.51loss_train: 169.6847accuracy_test: 0.4964loss_test: 16730.324\n",
      "\n",
      "\n",
      "accuracy_train: 0.6loss_train: 147.01074accuracy_test: 0.4807loss_test: 16053.799\n",
      "\n",
      "\n",
      "accuracy_train: 0.4loss_train: 159.50378accuracy_test: 0.3507loss_test: 16751.46\n",
      "\n",
      "\n",
      "accuracy_train: 0.48loss_train: 160.90451accuracy_test: 0.4579loss_test: 16648.943\n",
      "\n",
      "\n",
      "accuracy_train: 0.46loss_train: 156.24915accuracy_test: 0.4424loss_test: 15548.805\n",
      "\n",
      "\n",
      "accuracy_train: 0.5loss_train: 141.11658accuracy_test: 0.5356loss_test: 13680.599\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 117.22473accuracy_test: 0.675loss_test: 12043.797\n",
      "\n",
      "\n",
      "accuracy_train: 0.64loss_train: 109.1811accuracy_test: 0.5591loss_test: 12376.805\n",
      "\n",
      "\n",
      "accuracy_train: 0.62loss_train: 101.103195accuracy_test: 0.5726loss_test: 11957.148\n",
      "\n",
      "\n",
      "accuracy_train: 0.71loss_train: 110.27321accuracy_test: 0.6145loss_test: 12291.799\n",
      "\n",
      "\n",
      "accuracy_train: 0.68loss_train: 96.33913accuracy_test: 0.6489loss_test: 10755.396\n",
      "\n",
      "\n",
      "accuracy_train: 0.71loss_train: 80.37461accuracy_test: 0.6394loss_test: 10021.715\n",
      "\n",
      "\n",
      "accuracy_train: 0.71loss_train: 83.71838accuracy_test: 0.6435loss_test: 9503.202\n",
      "\n",
      "\n",
      "accuracy_train: 0.77loss_train: 85.716896accuracy_test: 0.702loss_test: 9284.873\n",
      "\n",
      "\n",
      "accuracy_train: 0.68loss_train: 89.61867accuracy_test: 0.6456loss_test: 10683.682\n",
      "\n",
      "\n",
      "accuracy_train: 0.53loss_train: 136.8587accuracy_test: 0.5828loss_test: 12170.893\n",
      "\n",
      "\n",
      "accuracy_train: 0.6loss_train: 117.78871accuracy_test: 0.5282loss_test: 13402.9795\n",
      "\n",
      "\n",
      "accuracy_train: 0.7loss_train: 101.26073accuracy_test: 0.7003loss_test: 10033.96\n",
      "\n",
      "\n",
      "accuracy_train: 0.72loss_train: 80.58456accuracy_test: 0.6658loss_test: 9039.476\n",
      "\n",
      "\n",
      "accuracy_train: 0.68loss_train: 90.573accuracy_test: 0.6655loss_test: 10326.53\n",
      "\n",
      "\n",
      "accuracy_train: 0.64loss_train: 105.22045accuracy_test: 0.6464loss_test: 10661.721\n",
      "\n",
      "\n",
      "accuracy_train: 0.7loss_train: 78.96326accuracy_test: 0.6358loss_test: 9727.074\n",
      "\n",
      "\n",
      "accuracy_train: 0.71loss_train: 85.24094accuracy_test: 0.6761loss_test: 9492.119\n",
      "\n",
      "\n",
      "accuracy_train: 0.67loss_train: 86.240845accuracy_test: 0.6455loss_test: 9793.193\n",
      "\n",
      "\n",
      "accuracy_train: 0.79loss_train: 62.60127accuracy_test: 0.7268loss_test: 9081.3955\n",
      "\n",
      "\n",
      "accuracy_train: 0.8loss_train: 70.67485accuracy_test: 0.7085loss_test: 8979.584\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 65.80141accuracy_test: 0.6899loss_test: 9750.855\n",
      "\n",
      "\n",
      "accuracy_train: 0.8loss_train: 76.90355accuracy_test: 0.7961loss_test: 7216.378\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 49.679794accuracy_test: 0.7053loss_test: 7789.093\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 75.49089accuracy_test: 0.7396loss_test: 7602.4697\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 51.239822accuracy_test: 0.8062loss_test: 6403.722\n",
      "\n",
      "\n",
      "accuracy_train: 0.8loss_train: 64.07876accuracy_test: 0.7195loss_test: 7823.66\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 62.934162accuracy_test: 0.7929loss_test: 7090.0576\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 42.086334accuracy_test: 0.8169loss_test: 5960.033\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 48.15657accuracy_test: 0.8163loss_test: 6072.7705\n",
      "\n",
      "\n",
      "accuracy_train: 0.72loss_train: 82.44853accuracy_test: 0.7351loss_test: 8136.396\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 61.295815accuracy_test: 0.7805loss_test: 6744.2163\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 49.811993accuracy_test: 0.7979loss_test: 6640.327\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 42.39701accuracy_test: 0.8248loss_test: 5672.3364\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 43.59527accuracy_test: 0.8212loss_test: 5655.675\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 40.589325accuracy_test: 0.8232loss_test: 5495.382\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 37.855225accuracy_test: 0.8424loss_test: 5206.4453\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 46.186237accuracy_test: 0.8119loss_test: 5718.911\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 51.383812accuracy_test: 0.7906loss_test: 6290.5596\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 33.92588accuracy_test: 0.8334loss_test: 5513.7363\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 31.599176accuracy_test: 0.8561loss_test: 4829.783\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 42.753036accuracy_test: 0.795loss_test: 6706.3447\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 38.8889accuracy_test: 0.8029loss_test: 6015.55\n",
      "\n",
      "\n",
      "accuracy_train: 0.79loss_train: 70.326614accuracy_test: 0.761loss_test: 8191.011\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 74.712585accuracy_test: 0.7815loss_test: 7119.7334\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 34.65978accuracy_test: 0.839loss_test: 5274.083\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 36.11663accuracy_test: 0.8518loss_test: 4851.734\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 24.435423accuracy_test: 0.8664loss_test: 4478.1104\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 35.94644accuracy_test: 0.858loss_test: 4642.61\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 39.43486accuracy_test: 0.866loss_test: 4569.9375\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 31.421368accuracy_test: 0.8528loss_test: 4753.4927\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 33.036293accuracy_test: 0.8686loss_test: 4473.61\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 29.724892accuracy_test: 0.878loss_test: 4167.8325\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 34.731503accuracy_test: 0.8559loss_test: 4660.456\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 21.781017accuracy_test: 0.885loss_test: 4064.4758\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 34.1184accuracy_test: 0.8913loss_test: 3806.4192\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 31.387413accuracy_test: 0.8762loss_test: 4272.083\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 26.832436accuracy_test: 0.8793loss_test: 4057.7024\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 33.039406accuracy_test: 0.8616loss_test: 4275.0376\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 23.480618accuracy_test: 0.8682loss_test: 4283.229\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 26.649563accuracy_test: 0.8661loss_test: 4386.798\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 17.855127accuracy_test: 0.8695loss_test: 4272.2295\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 29.66497accuracy_test: 0.8794loss_test: 4006.5933\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 22.54572accuracy_test: 0.8713loss_test: 4137.59\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 27.081812accuracy_test: 0.8772loss_test: 3942.505\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 20.36881accuracy_test: 0.8879loss_test: 3802.4634\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 14.668148accuracy_test: 0.8852loss_test: 3855.0115\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 17.367626accuracy_test: 0.8853loss_test: 3752.228\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 25.867523accuracy_test: 0.8714loss_test: 4209.79\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 18.92622accuracy_test: 0.892loss_test: 3572.9614\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 11.665322accuracy_test: 0.8855loss_test: 3734.227\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 36.53144accuracy_test: 0.8766loss_test: 4045.4026\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 25.756248accuracy_test: 0.8879loss_test: 3640.7622\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 13.854532accuracy_test: 0.8915loss_test: 3694.658\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 22.655596accuracy_test: 0.8789loss_test: 3943.9426\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 22.53336accuracy_test: 0.8931loss_test: 3647.9773\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 21.877438accuracy_test: 0.899loss_test: 3423.734\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 21.13751accuracy_test: 0.8875loss_test: 3790.2705\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 37.3413accuracy_test: 0.8784loss_test: 3919.4011\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 12.240576accuracy_test: 0.9031loss_test: 3364.1672\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 17.308594accuracy_test: 0.9007loss_test: 3411.4312\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 24.530472accuracy_test: 0.9033loss_test: 3350.0425\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.970095accuracy_test: 0.8883loss_test: 3722.2139\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 14.721872accuracy_test: 0.8884loss_test: 3704.667\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 24.136246accuracy_test: 0.8873loss_test: 3771.6074\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200] ,stddev = 0.1))\n",
    "b1 = tf.Variable(tf.zeros([200]))\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([200,10], stddev = 0.1))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "init = tf.global_variables_initializer()           \n",
    "\n",
    "#the model\n",
    "#using sigmoid on the first layer and softmax onthe output layer\n",
    "XX = tf.reshape(X,[-1,784])\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1)+b1)\n",
    "\n",
    "Y  = tf.nn.softmax(tf.matmul(Y1, W2)+b2)  \n",
    "\n",
    "\n",
    "Y_ = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "#loss function\n",
    "cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y))  \n",
    "            \n",
    "#% of correct answers found in the batch\n",
    "is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "  \n",
    "#performing gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "                \n",
    "#executing the training \n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(100):\n",
    "    # load batch of images and correct answers\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    train_data={X: batch_X, Y_: batch_Y}\n",
    "\n",
    "    # train\n",
    "    sess.run(train_step, feed_dict = train_data)\n",
    "    \n",
    "    #succes\n",
    "    a_train, c_train = sess.run([accuracy, cross_entropy], feed_dict=train_data)\n",
    "                \n",
    "   \n",
    "    # success on test data ?\n",
    "    test_data={X: mnist.test.images, Y_: mnist.test.labels}\n",
    "    a_test,c_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "    \n",
    "    print('accuracy_train: ' + str(a_train) + 'loss_train: ' + str(c_train) + 'accuracy_test: ' + str(a_test) + 'loss_test: ' + str(c_test))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1529,
     "status": "ok",
     "timestamp": 1539948195574,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "FdlsdMGnCx_S",
    "outputId": "3ac34920-40f4-4644-ed84-e18a7d7f5b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is:  88.73%\n"
     ]
    }
   ],
   "source": [
    "a_test, c_test  = a_test,c_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "\n",
    "print('Test accuracy is: ', (\"%.2f\"%(a_test*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LDENQUJU1M_z"
   },
   "source": [
    "# **with sigmoid function , five hidden layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 21350
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41755,
     "status": "ok",
     "timestamp": 1539948904987,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "G6qY4d1f1HRn",
    "outputId": "d14c55c4-ecbb-42f6-d616-48d562d79dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_train: 0.14loss_train: 229.34673accuracy_test: 0.0892loss_test: 231.43315\n",
      "\n",
      "\n",
      "accuracy_train: 0.06loss_train: 230.79901accuracy_test: 0.0892loss_test: 231.04855\n",
      "\n",
      "\n",
      "accuracy_train: 0.08loss_train: 229.97423accuracy_test: 0.098loss_test: 230.86429\n",
      "\n",
      "\n",
      "accuracy_train: 0.22loss_train: 229.73299accuracy_test: 0.1905loss_test: 230.80524\n",
      "\n",
      "\n",
      "accuracy_train: 0.19loss_train: 227.35484accuracy_test: 0.1135loss_test: 230.83215\n",
      "\n",
      "\n",
      "accuracy_train: 0.12loss_train: 229.68033accuracy_test: 0.1135loss_test: 230.93777\n",
      "\n",
      "\n",
      "accuracy_train: 0.1loss_train: 232.59671accuracy_test: 0.1135loss_test: 230.8618\n",
      "\n",
      "\n",
      "accuracy_train: 0.1loss_train: 230.7235accuracy_test: 0.1135loss_test: 230.74129\n",
      "\n",
      "\n",
      "accuracy_train: 0.11loss_train: 230.55215accuracy_test: 0.1135loss_test: 230.60007\n",
      "\n",
      "\n",
      "accuracy_train: 0.09loss_train: 231.67534accuracy_test: 0.1135loss_test: 230.37091\n",
      "\n",
      "\n",
      "accuracy_train: 0.12loss_train: 230.4425accuracy_test: 0.1135loss_test: 230.11406\n",
      "\n",
      "\n",
      "accuracy_train: 0.18loss_train: 227.80367accuracy_test: 0.1135loss_test: 229.88116\n",
      "\n",
      "\n",
      "accuracy_train: 0.14loss_train: 228.11723accuracy_test: 0.1135loss_test: 229.64091\n",
      "\n",
      "\n",
      "accuracy_train: 0.08loss_train: 229.72475accuracy_test: 0.1135loss_test: 229.37985\n",
      "\n",
      "\n",
      "accuracy_train: 0.12loss_train: 228.3777accuracy_test: 0.1135loss_test: 229.14212\n",
      "\n",
      "\n",
      "accuracy_train: 0.14loss_train: 228.34879accuracy_test: 0.1135loss_test: 228.88095\n",
      "\n",
      "\n",
      "accuracy_train: 0.09loss_train: 230.31607accuracy_test: 0.1135loss_test: 228.55489\n",
      "\n",
      "\n",
      "accuracy_train: 0.1loss_train: 228.41978accuracy_test: 0.1135loss_test: 228.21472\n",
      "\n",
      "\n",
      "accuracy_train: 0.08loss_train: 228.98154accuracy_test: 0.1158loss_test: 227.84097\n",
      "\n",
      "\n",
      "accuracy_train: 0.13loss_train: 229.16267accuracy_test: 0.1751loss_test: 227.49805\n",
      "\n",
      "\n",
      "accuracy_train: 0.23loss_train: 226.08319accuracy_test: 0.1891loss_test: 227.09215\n",
      "\n",
      "\n",
      "accuracy_train: 0.28loss_train: 225.54492accuracy_test: 0.2269loss_test: 226.60437\n",
      "\n",
      "\n",
      "accuracy_train: 0.21loss_train: 226.44954accuracy_test: 0.192loss_test: 226.08449\n",
      "\n",
      "\n",
      "accuracy_train: 0.2loss_train: 225.76709accuracy_test: 0.1958loss_test: 225.55301\n",
      "\n",
      "\n",
      "accuracy_train: 0.19loss_train: 226.56978accuracy_test: 0.1968loss_test: 224.96793\n",
      "\n",
      "\n",
      "accuracy_train: 0.2loss_train: 224.0791accuracy_test: 0.1969loss_test: 224.33751\n",
      "\n",
      "\n",
      "accuracy_train: 0.16loss_train: 224.56494accuracy_test: 0.1964loss_test: 223.6104\n",
      "\n",
      "\n",
      "accuracy_train: 0.21loss_train: 222.92488accuracy_test: 0.1973loss_test: 222.84286\n",
      "\n",
      "\n",
      "accuracy_train: 0.16loss_train: 223.62738accuracy_test: 0.2087loss_test: 221.97325\n",
      "\n",
      "\n",
      "accuracy_train: 0.22loss_train: 221.2394accuracy_test: 0.2329loss_test: 221.00012\n",
      "\n",
      "\n",
      "accuracy_train: 0.33loss_train: 220.16936accuracy_test: 0.338loss_test: 219.99884\n",
      "\n",
      "\n",
      "accuracy_train: 0.4loss_train: 217.823accuracy_test: 0.3516loss_test: 218.9375\n",
      "\n",
      "\n",
      "accuracy_train: 0.3loss_train: 217.6316accuracy_test: 0.3319loss_test: 217.8757\n",
      "\n",
      "\n",
      "accuracy_train: 0.34loss_train: 215.36214accuracy_test: 0.2972loss_test: 216.67374\n",
      "\n",
      "\n",
      "accuracy_train: 0.24loss_train: 217.14807accuracy_test: 0.2716loss_test: 215.4073\n",
      "\n",
      "\n",
      "accuracy_train: 0.31loss_train: 212.86557accuracy_test: 0.2704loss_test: 214.00879\n",
      "\n",
      "\n",
      "accuracy_train: 0.24loss_train: 212.71523accuracy_test: 0.2644loss_test: 212.6024\n",
      "\n",
      "\n",
      "accuracy_train: 0.26loss_train: 211.94353accuracy_test: 0.2561loss_test: 211.19547\n",
      "\n",
      "\n",
      "accuracy_train: 0.23loss_train: 210.49034accuracy_test: 0.2512loss_test: 209.76923\n",
      "\n",
      "\n",
      "accuracy_train: 0.25loss_train: 210.42227accuracy_test: 0.2472loss_test: 208.3195\n",
      "\n",
      "\n",
      "accuracy_train: 0.23loss_train: 209.2149accuracy_test: 0.2485loss_test: 206.85463\n",
      "\n",
      "\n",
      "accuracy_train: 0.29loss_train: 204.97205accuracy_test: 0.285loss_test: 205.42065\n",
      "\n",
      "\n",
      "accuracy_train: 0.3loss_train: 204.29532accuracy_test: 0.3173loss_test: 204.0006\n",
      "\n",
      "\n",
      "accuracy_train: 0.29loss_train: 202.82314accuracy_test: 0.3201loss_test: 202.5773\n",
      "\n",
      "\n",
      "accuracy_train: 0.35loss_train: 199.2575accuracy_test: 0.3219loss_test: 201.11537\n",
      "\n",
      "\n",
      "accuracy_train: 0.25loss_train: 203.17206accuracy_test: 0.326loss_test: 199.66617\n",
      "\n",
      "\n",
      "accuracy_train: 0.33loss_train: 195.66539accuracy_test: 0.3286loss_test: 198.31793\n",
      "\n",
      "\n",
      "accuracy_train: 0.35loss_train: 195.03178accuracy_test: 0.3146loss_test: 197.09459\n",
      "\n",
      "\n",
      "accuracy_train: 0.32loss_train: 191.469accuracy_test: 0.2885loss_test: 195.79488\n",
      "\n",
      "\n",
      "accuracy_train: 0.26loss_train: 192.66353accuracy_test: 0.2463loss_test: 194.24461\n",
      "\n",
      "\n",
      "accuracy_train: 0.23loss_train: 193.7815accuracy_test: 0.2622loss_test: 192.68033\n",
      "\n",
      "\n",
      "accuracy_train: 0.21loss_train: 193.69275accuracy_test: 0.2852loss_test: 191.4811\n",
      "\n",
      "\n",
      "accuracy_train: 0.31loss_train: 191.60999accuracy_test: 0.311loss_test: 190.3648\n",
      "\n",
      "\n",
      "accuracy_train: 0.37loss_train: 189.45293accuracy_test: 0.3381loss_test: 189.26497\n",
      "\n",
      "\n",
      "accuracy_train: 0.44loss_train: 186.50526accuracy_test: 0.3489loss_test: 188.16527\n",
      "\n",
      "\n",
      "accuracy_train: 0.26loss_train: 192.43472accuracy_test: 0.3404loss_test: 186.80887\n",
      "\n",
      "\n",
      "accuracy_train: 0.27loss_train: 187.092accuracy_test: 0.3173loss_test: 185.0203\n",
      "\n",
      "\n",
      "accuracy_train: 0.32loss_train: 182.29996accuracy_test: 0.3033loss_test: 183.69817\n",
      "\n",
      "\n",
      "accuracy_train: 0.24loss_train: 188.24406accuracy_test: 0.3086loss_test: 183.35219\n",
      "\n",
      "\n",
      "accuracy_train: 0.31loss_train: 176.11748accuracy_test: 0.3214loss_test: 183.51465\n",
      "\n",
      "\n",
      "accuracy_train: 0.33loss_train: 175.05609accuracy_test: 0.3288loss_test: 182.4811\n",
      "\n",
      "\n",
      "accuracy_train: 0.44loss_train: 176.23473accuracy_test: 0.3305loss_test: 180.2648\n",
      "\n",
      "\n",
      "accuracy_train: 0.28loss_train: 173.8148accuracy_test: 0.3328loss_test: 177.9004\n",
      "\n",
      "\n",
      "accuracy_train: 0.28loss_train: 178.27173accuracy_test: 0.3381loss_test: 176.64058\n",
      "\n",
      "\n",
      "accuracy_train: 0.41loss_train: 168.21275accuracy_test: 0.3437loss_test: 175.83691\n",
      "\n",
      "\n",
      "accuracy_train: 0.3loss_train: 178.00049accuracy_test: 0.3554loss_test: 174.96025\n",
      "\n",
      "\n",
      "accuracy_train: 0.34loss_train: 179.23692accuracy_test: 0.3652loss_test: 173.77985\n",
      "\n",
      "\n",
      "accuracy_train: 0.35loss_train: 170.46884accuracy_test: 0.3692loss_test: 172.54152\n",
      "\n",
      "\n",
      "accuracy_train: 0.31loss_train: 173.19238accuracy_test: 0.3935loss_test: 171.46382\n",
      "\n",
      "\n",
      "accuracy_train: 0.41loss_train: 168.65619accuracy_test: 0.4461loss_test: 170.66454\n",
      "\n",
      "\n",
      "accuracy_train: 0.46loss_train: 172.29172accuracy_test: 0.4762loss_test: 169.82745\n",
      "\n",
      "\n",
      "accuracy_train: 0.39loss_train: 170.593accuracy_test: 0.4698loss_test: 168.7515\n",
      "\n",
      "\n",
      "accuracy_train: 0.45loss_train: 167.59636accuracy_test: 0.4531loss_test: 167.57387\n",
      "\n",
      "\n",
      "accuracy_train: 0.48loss_train: 165.40726accuracy_test: 0.4589loss_test: 166.41374\n",
      "\n",
      "\n",
      "accuracy_train: 0.49loss_train: 165.72415accuracy_test: 0.478loss_test: 165.27118\n",
      "\n",
      "\n",
      "accuracy_train: 0.53loss_train: 161.10516accuracy_test: 0.4969loss_test: 164.26332\n",
      "\n",
      "\n",
      "accuracy_train: 0.45loss_train: 171.35109accuracy_test: 0.5006loss_test: 163.49599\n",
      "\n",
      "\n",
      "accuracy_train: 0.47loss_train: 167.70314accuracy_test: 0.487loss_test: 162.87398\n",
      "\n",
      "\n",
      "accuracy_train: 0.48loss_train: 162.05946accuracy_test: 0.4764loss_test: 162.32648\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 154.95007accuracy_test: 0.4804loss_test: 161.973\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 157.1985accuracy_test: 0.5017loss_test: 161.28859\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 162.24394accuracy_test: 0.5238loss_test: 159.98059\n",
      "\n",
      "\n",
      "accuracy_train: 0.43loss_train: 162.89368accuracy_test: 0.5408loss_test: 158.5552\n",
      "\n",
      "\n",
      "accuracy_train: 0.54loss_train: 160.63585accuracy_test: 0.5428loss_test: 157.5161\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 163.56694accuracy_test: 0.5418loss_test: 156.6321\n",
      "\n",
      "\n",
      "accuracy_train: 0.55loss_train: 158.10498accuracy_test: 0.5445loss_test: 155.65158\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 161.12912accuracy_test: 0.5481loss_test: 154.4318\n",
      "\n",
      "\n",
      "accuracy_train: 0.55loss_train: 153.20372accuracy_test: 0.547loss_test: 153.2667\n",
      "\n",
      "\n",
      "accuracy_train: 0.61loss_train: 152.69441accuracy_test: 0.5418loss_test: 152.17531\n",
      "\n",
      "\n",
      "accuracy_train: 0.56loss_train: 149.54321accuracy_test: 0.5379loss_test: 151.14983\n",
      "\n",
      "\n",
      "accuracy_train: 0.56loss_train: 150.23822accuracy_test: 0.5358loss_test: 150.38783\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 145.96626accuracy_test: 0.5305loss_test: 149.81154\n",
      "\n",
      "\n",
      "accuracy_train: 0.55loss_train: 146.71103accuracy_test: 0.5247loss_test: 149.21817\n",
      "\n",
      "\n",
      "accuracy_train: 0.48loss_train: 152.07751accuracy_test: 0.5259loss_test: 148.48442\n",
      "\n",
      "\n",
      "accuracy_train: 0.61loss_train: 148.16356accuracy_test: 0.5246loss_test: 147.66039\n",
      "\n",
      "\n",
      "accuracy_train: 0.47loss_train: 153.69035accuracy_test: 0.5251loss_test: 146.51762\n",
      "\n",
      "\n",
      "accuracy_train: 0.49loss_train: 139.66464accuracy_test: 0.5238loss_test: 145.46086\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 137.61902accuracy_test: 0.5271loss_test: 144.59923\n",
      "\n",
      "\n",
      "accuracy_train: 0.53loss_train: 142.47478accuracy_test: 0.5267loss_test: 144.0748\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 146.08755accuracy_test: 0.5202loss_test: 143.90164\n",
      "\n",
      "\n",
      "accuracy_train: 0.48loss_train: 147.09477accuracy_test: 0.5281loss_test: 143.21208\n",
      "\n",
      "\n",
      "accuracy_train: 0.64loss_train: 132.14183accuracy_test: 0.5626loss_test: 141.6207\n",
      "\n",
      "\n",
      "accuracy_train: 0.59loss_train: 129.2808accuracy_test: 0.5973loss_test: 140.41461\n",
      "\n",
      "\n",
      "accuracy_train: 0.55loss_train: 147.52812accuracy_test: 0.6107loss_test: 139.40889\n",
      "\n",
      "\n",
      "accuracy_train: 0.68loss_train: 129.07373accuracy_test: 0.615loss_test: 138.88608\n",
      "\n",
      "\n",
      "accuracy_train: 0.59loss_train: 142.71948accuracy_test: 0.6117loss_test: 138.48225\n",
      "\n",
      "\n",
      "accuracy_train: 0.71loss_train: 127.71011accuracy_test: 0.6075loss_test: 138.20042\n",
      "\n",
      "\n",
      "accuracy_train: 0.62loss_train: 140.48158accuracy_test: 0.6093loss_test: 137.59174\n",
      "\n",
      "\n",
      "accuracy_train: 0.72loss_train: 123.6944accuracy_test: 0.6118loss_test: 136.83119\n",
      "\n",
      "\n",
      "accuracy_train: 0.63loss_train: 132.47658accuracy_test: 0.6107loss_test: 135.93333\n",
      "\n",
      "\n",
      "accuracy_train: 0.63loss_train: 133.64258accuracy_test: 0.614loss_test: 134.9837\n",
      "\n",
      "\n",
      "accuracy_train: 0.66loss_train: 131.51465accuracy_test: 0.6155loss_test: 134.33391\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 122.22956accuracy_test: 0.6165loss_test: 133.71992\n",
      "\n",
      "\n",
      "accuracy_train: 0.58loss_train: 129.06354accuracy_test: 0.6285loss_test: 132.70294\n",
      "\n",
      "\n",
      "accuracy_train: 0.63loss_train: 130.15715accuracy_test: 0.6412loss_test: 131.78854\n",
      "\n",
      "\n",
      "accuracy_train: 0.7loss_train: 118.45247accuracy_test: 0.6436loss_test: 131.142\n",
      "\n",
      "\n",
      "accuracy_train: 0.59loss_train: 130.39543accuracy_test: 0.6479loss_test: 130.51082\n",
      "\n",
      "\n",
      "accuracy_train: 0.67loss_train: 117.039764accuracy_test: 0.6522loss_test: 129.86469\n",
      "\n",
      "\n",
      "accuracy_train: 0.69loss_train: 133.04773accuracy_test: 0.6543loss_test: 129.22649\n",
      "\n",
      "\n",
      "accuracy_train: 0.65loss_train: 137.1438accuracy_test: 0.6572loss_test: 128.67563\n",
      "\n",
      "\n",
      "accuracy_train: 0.62loss_train: 138.60759accuracy_test: 0.6563loss_test: 128.30716\n",
      "\n",
      "\n",
      "accuracy_train: 0.64loss_train: 126.76461accuracy_test: 0.6545loss_test: 127.9917\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 121.76215accuracy_test: 0.6597loss_test: 127.25631\n",
      "\n",
      "\n",
      "accuracy_train: 0.66loss_train: 123.72392accuracy_test: 0.6697loss_test: 126.37268\n",
      "\n",
      "\n",
      "accuracy_train: 0.63loss_train: 131.17007accuracy_test: 0.6723loss_test: 125.40878\n",
      "\n",
      "\n",
      "accuracy_train: 0.69loss_train: 119.69246accuracy_test: 0.6784loss_test: 124.673065\n",
      "\n",
      "\n",
      "accuracy_train: 0.75loss_train: 113.82692accuracy_test: 0.6746loss_test: 124.174355\n",
      "\n",
      "\n",
      "accuracy_train: 0.65loss_train: 130.95828accuracy_test: 0.6697loss_test: 123.638466\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 122.347404accuracy_test: 0.6642loss_test: 123.1881\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 113.38464accuracy_test: 0.6607loss_test: 122.833725\n",
      "\n",
      "\n",
      "accuracy_train: 0.64loss_train: 119.67358accuracy_test: 0.6581loss_test: 122.22877\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 111.44078accuracy_test: 0.655loss_test: 121.39586\n",
      "\n",
      "\n",
      "accuracy_train: 0.65loss_train: 128.06934accuracy_test: 0.6464loss_test: 120.99061\n",
      "\n",
      "\n",
      "accuracy_train: 0.71loss_train: 107.27914accuracy_test: 0.6412loss_test: 120.89954\n",
      "\n",
      "\n",
      "accuracy_train: 0.66loss_train: 119.923904accuracy_test: 0.6429loss_test: 120.6018\n",
      "\n",
      "\n",
      "accuracy_train: 0.7loss_train: 119.5411accuracy_test: 0.6479loss_test: 120.24604\n",
      "\n",
      "\n",
      "accuracy_train: 0.7loss_train: 108.8757accuracy_test: 0.656loss_test: 119.76614\n",
      "\n",
      "\n",
      "accuracy_train: 0.67loss_train: 120.118866accuracy_test: 0.6681loss_test: 118.863655\n",
      "\n",
      "\n",
      "accuracy_train: 0.61loss_train: 121.42938accuracy_test: 0.6755loss_test: 117.82533\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 106.91036accuracy_test: 0.6783loss_test: 117.19411\n",
      "\n",
      "\n",
      "accuracy_train: 0.75loss_train: 109.477425accuracy_test: 0.6805loss_test: 116.73733\n",
      "\n",
      "\n",
      "accuracy_train: 0.64loss_train: 117.4743accuracy_test: 0.6824loss_test: 116.34692\n",
      "\n",
      "\n",
      "accuracy_train: 0.64loss_train: 114.732704accuracy_test: 0.6843loss_test: 115.90476\n",
      "\n",
      "\n",
      "accuracy_train: 0.72loss_train: 112.21503accuracy_test: 0.688loss_test: 114.9111\n",
      "\n",
      "\n",
      "accuracy_train: 0.68loss_train: 119.39222accuracy_test: 0.6915loss_test: 114.09442\n",
      "\n",
      "\n",
      "accuracy_train: 0.67loss_train: 114.08372accuracy_test: 0.697loss_test: 113.36691\n",
      "\n",
      "\n",
      "accuracy_train: 0.77loss_train: 97.601395accuracy_test: 0.6961loss_test: 112.9635\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 104.89348accuracy_test: 0.6991loss_test: 112.465965\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 103.762436accuracy_test: 0.7032loss_test: 111.844\n",
      "\n",
      "\n",
      "accuracy_train: 0.71loss_train: 115.54905accuracy_test: 0.7113loss_test: 110.93085\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 106.25704accuracy_test: 0.713loss_test: 110.41876\n",
      "\n",
      "\n",
      "accuracy_train: 0.79loss_train: 101.03058accuracy_test: 0.715loss_test: 110.28839\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 110.49452accuracy_test: 0.7074loss_test: 110.478714\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 113.76126accuracy_test: 0.7043loss_test: 110.49305\n",
      "\n",
      "\n",
      "accuracy_train: 0.7loss_train: 118.638084accuracy_test: 0.7142loss_test: 109.380844\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 108.59543accuracy_test: 0.7259loss_test: 108.08163\n",
      "\n",
      "\n",
      "accuracy_train: 0.69loss_train: 100.78384accuracy_test: 0.7397loss_test: 106.8758\n",
      "\n",
      "\n",
      "accuracy_train: 0.75loss_train: 118.06606accuracy_test: 0.7469loss_test: 106.37949\n",
      "\n",
      "\n",
      "accuracy_train: 0.77loss_train: 99.417595accuracy_test: 0.7377loss_test: 107.04113\n",
      "\n",
      "\n",
      "accuracy_train: 0.67loss_train: 114.07556accuracy_test: 0.7252loss_test: 108.05104\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 100.30098accuracy_test: 0.7245loss_test: 108.053696\n",
      "\n",
      "\n",
      "accuracy_train: 0.7loss_train: 108.62973accuracy_test: 0.7381loss_test: 106.05456\n",
      "\n",
      "\n",
      "accuracy_train: 0.69loss_train: 112.63728accuracy_test: 0.7518loss_test: 104.13629\n",
      "\n",
      "\n",
      "accuracy_train: 0.72loss_train: 103.53385accuracy_test: 0.7561loss_test: 103.10196\n",
      "\n",
      "\n",
      "accuracy_train: 0.63loss_train: 115.236664accuracy_test: 0.7578loss_test: 102.69496\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 92.898026accuracy_test: 0.7564loss_test: 102.460014\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 98.73333accuracy_test: 0.7601loss_test: 102.18332\n",
      "\n",
      "\n",
      "accuracy_train: 0.75loss_train: 103.71904accuracy_test: 0.7627loss_test: 101.92562\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 94.08639accuracy_test: 0.764loss_test: 102.12406\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 110.97588accuracy_test: 0.7687loss_test: 101.96451\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 99.30183accuracy_test: 0.776loss_test: 100.88726\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 98.84419accuracy_test: 0.7814loss_test: 99.75889\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 93.52795accuracy_test: 0.7856loss_test: 98.7245\n",
      "\n",
      "\n",
      "accuracy_train: 0.8loss_train: 98.81838accuracy_test: 0.7822loss_test: 97.98293\n",
      "\n",
      "\n",
      "accuracy_train: 0.77loss_train: 98.08284accuracy_test: 0.7783loss_test: 97.67427\n",
      "\n",
      "\n",
      "accuracy_train: 0.75loss_train: 102.08867accuracy_test: 0.7683loss_test: 97.811\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 97.343765accuracy_test: 0.7681loss_test: 97.493576\n",
      "\n",
      "\n",
      "accuracy_train: 0.79loss_train: 104.41631accuracy_test: 0.7585loss_test: 98.14969\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 107.88379accuracy_test: 0.7533loss_test: 98.96771\n",
      "\n",
      "\n",
      "accuracy_train: 0.75loss_train: 108.30161accuracy_test: 0.7469loss_test: 100.06733\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 105.13411accuracy_test: 0.7522loss_test: 99.56811\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 96.55243accuracy_test: 0.766loss_test: 98.10211\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 90.21731accuracy_test: 0.7852loss_test: 95.39527\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 89.37801accuracy_test: 0.7954loss_test: 93.40115\n",
      "\n",
      "\n",
      "accuracy_train: 0.75loss_train: 96.16986accuracy_test: 0.7844loss_test: 93.86263\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 106.797966accuracy_test: 0.7746loss_test: 94.73169\n",
      "\n",
      "\n",
      "accuracy_train: 0.72loss_train: 99.3254accuracy_test: 0.7695loss_test: 94.966896\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 97.74639accuracy_test: 0.782loss_test: 93.16593\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 77.84007accuracy_test: 0.7925loss_test: 91.53377\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 92.78019accuracy_test: 0.7968loss_test: 90.72092\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 88.795494accuracy_test: 0.7949loss_test: 90.22631\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 86.436485accuracy_test: 0.7926loss_test: 90.133675\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 90.47273accuracy_test: 0.7935loss_test: 89.5921\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 84.72926accuracy_test: 0.7989loss_test: 88.55748\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 78.43962accuracy_test: 0.804loss_test: 87.47533\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 74.71595accuracy_test: 0.8028loss_test: 87.023865\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 82.03466accuracy_test: 0.8044loss_test: 86.89084\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 86.44083accuracy_test: 0.8052loss_test: 86.70959\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 87.987816accuracy_test: 0.8041loss_test: 86.34217\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 84.87845accuracy_test: 0.8068loss_test: 86.00964\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 77.65541accuracy_test: 0.8084loss_test: 85.38693\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 88.57298accuracy_test: 0.8028loss_test: 85.28511\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 74.835686accuracy_test: 0.8051loss_test: 84.798225\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 86.12485accuracy_test: 0.808loss_test: 83.8081\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 77.44425accuracy_test: 0.8108loss_test: 82.59897\n",
      "\n",
      "\n",
      "accuracy_train: 0.8loss_train: 76.91963accuracy_test: 0.8074loss_test: 82.79699\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 75.42501accuracy_test: 0.7992loss_test: 83.69812\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 92.08844accuracy_test: 0.7936loss_test: 83.58973\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 83.64896accuracy_test: 0.7997loss_test: 82.04947\n",
      "\n",
      "\n",
      "accuracy_train: 0.77loss_train: 81.36563accuracy_test: 0.807loss_test: 80.92972\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 95.58394accuracy_test: 0.8112loss_test: 80.359856\n",
      "\n",
      "\n",
      "accuracy_train: 0.79loss_train: 80.0893accuracy_test: 0.8132loss_test: 80.606834\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 72.66009accuracy_test: 0.8177loss_test: 80.10323\n",
      "\n",
      "\n",
      "accuracy_train: 0.79loss_train: 77.168274accuracy_test: 0.8213loss_test: 79.709885\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 72.629036accuracy_test: 0.8146loss_test: 79.561806\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 80.99663accuracy_test: 0.809loss_test: 79.699066\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 80.481094accuracy_test: 0.8072loss_test: 79.86109\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 88.943405accuracy_test: 0.8121loss_test: 79.25369\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 82.68114accuracy_test: 0.8175loss_test: 78.216866\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 85.02595accuracy_test: 0.8268loss_test: 76.9106\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 79.55652accuracy_test: 0.8252loss_test: 76.61471\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 86.93591accuracy_test: 0.8194loss_test: 76.910965\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 71.1018accuracy_test: 0.8088loss_test: 77.743385\n",
      "\n",
      "\n",
      "accuracy_train: 0.77loss_train: 78.33869accuracy_test: 0.8055loss_test: 78.03\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 75.56088accuracy_test: 0.8089loss_test: 77.26958\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 72.797646accuracy_test: 0.817loss_test: 76.29739\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 68.58233accuracy_test: 0.8253loss_test: 75.39666\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 78.99536accuracy_test: 0.8314loss_test: 75.07231\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 69.05534accuracy_test: 0.833loss_test: 74.91712\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 77.78435accuracy_test: 0.8303loss_test: 75.26045\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 73.46227accuracy_test: 0.828loss_test: 75.35876\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 74.98456accuracy_test: 0.8273loss_test: 74.878456\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 75.71031accuracy_test: 0.8294loss_test: 73.45857\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 64.768845accuracy_test: 0.8358loss_test: 72.11858\n",
      "\n",
      "\n",
      "accuracy_train: 0.8loss_train: 71.579285accuracy_test: 0.8366loss_test: 71.49929\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 78.21862accuracy_test: 0.8326loss_test: 71.54839\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 81.25978accuracy_test: 0.8269loss_test: 72.19723\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 82.756714accuracy_test: 0.8196loss_test: 73.04554\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 59.266136accuracy_test: 0.8145loss_test: 73.587746\n",
      "\n",
      "\n",
      "accuracy_train: 0.78loss_train: 84.826126accuracy_test: 0.8123loss_test: 73.37343\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 68.35468accuracy_test: 0.8147loss_test: 71.71121\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 83.43635accuracy_test: 0.8176loss_test: 70.42562\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 74.65511accuracy_test: 0.8168loss_test: 69.86959\n",
      "\n",
      "\n",
      "accuracy_train: 0.79loss_train: 78.851006accuracy_test: 0.8135loss_test: 69.71734\n",
      "\n",
      "\n",
      "accuracy_train: 0.8loss_train: 71.89186accuracy_test: 0.8181loss_test: 69.290474\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 61.95705accuracy_test: 0.8191loss_test: 69.122025\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 70.58227accuracy_test: 0.8202loss_test: 68.98124\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 61.089195accuracy_test: 0.8241loss_test: 68.17337\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 60.77382accuracy_test: 0.8305loss_test: 67.513664\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 73.05963accuracy_test: 0.8341loss_test: 67.31169\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 53.994476accuracy_test: 0.8375loss_test: 67.39764\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 59.488148accuracy_test: 0.8337loss_test: 67.58342\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 70.891495accuracy_test: 0.8325loss_test: 67.62125\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 50.857044accuracy_test: 0.831loss_test: 67.60601\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 67.62011accuracy_test: 0.8355loss_test: 67.21811\n",
      "\n",
      "\n",
      "accuracy_train: 0.73loss_train: 87.92875accuracy_test: 0.8454loss_test: 66.29307\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 68.41556accuracy_test: 0.8445loss_test: 66.4305\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 67.416084accuracy_test: 0.8366loss_test: 67.23631\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 69.516365accuracy_test: 0.8336loss_test: 67.13061\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 66.32224accuracy_test: 0.8331loss_test: 66.90252\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 67.389496accuracy_test: 0.8509loss_test: 64.7643\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 64.406906accuracy_test: 0.8607loss_test: 64.144745\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 60.240025accuracy_test: 0.8653loss_test: 64.77533\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 65.50761accuracy_test: 0.8639loss_test: 64.82383\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 66.32999accuracy_test: 0.8663loss_test: 63.706036\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 69.62101accuracy_test: 0.8671loss_test: 62.995953\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 62.34545accuracy_test: 0.8651loss_test: 62.54693\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 57.321827accuracy_test: 0.865loss_test: 62.29672\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 60.714977accuracy_test: 0.8589loss_test: 62.180412\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 62.33245accuracy_test: 0.8565loss_test: 62.477077\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 60.29979accuracy_test: 0.8554loss_test: 62.265694\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 47.864143accuracy_test: 0.8571loss_test: 61.670513\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 52.9178accuracy_test: 0.8625loss_test: 61.151688\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 49.20288accuracy_test: 0.8626loss_test: 61.13188\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 47.527443accuracy_test: 0.8628loss_test: 61.36992\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 63.774197accuracy_test: 0.8624loss_test: 61.260933\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 70.781944accuracy_test: 0.8611loss_test: 61.033047\n",
      "\n",
      "\n",
      "accuracy_train: 0.8loss_train: 69.50228accuracy_test: 0.8588loss_test: 60.59599\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 50.327778accuracy_test: 0.8566loss_test: 60.264652\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 48.128235accuracy_test: 0.858loss_test: 59.883995\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 53.450882accuracy_test: 0.8583loss_test: 59.33787\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 60.943485accuracy_test: 0.8568loss_test: 59.069847\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 55.78821accuracy_test: 0.8565loss_test: 59.12887\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 53.601486accuracy_test: 0.8635loss_test: 59.247623\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 70.208984accuracy_test: 0.868loss_test: 59.08915\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 65.07939accuracy_test: 0.8743loss_test: 57.93281\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 41.56598accuracy_test: 0.8772loss_test: 57.279915\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 52.932884accuracy_test: 0.8738loss_test: 57.187607\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 69.34549accuracy_test: 0.8689loss_test: 57.256985\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 40.517235accuracy_test: 0.8666loss_test: 57.3125\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 50.326782accuracy_test: 0.8661loss_test: 56.726635\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 52.09778accuracy_test: 0.8657loss_test: 56.27733\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 53.263474accuracy_test: 0.8653loss_test: 55.76423\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 60.115154accuracy_test: 0.8668loss_test: 55.60039\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 49.025326accuracy_test: 0.8703loss_test: 55.93607\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 49.612953accuracy_test: 0.8682loss_test: 56.672413\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 46.73914accuracy_test: 0.8653loss_test: 57.83312\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 59.521286accuracy_test: 0.8636loss_test: 58.049046\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 55.784946accuracy_test: 0.8633loss_test: 57.917183\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 43.571423accuracy_test: 0.8686loss_test: 56.787918\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 58.58207accuracy_test: 0.8715loss_test: 55.299522\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 51.278667accuracy_test: 0.8735loss_test: 54.588104\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 45.94161accuracy_test: 0.8718loss_test: 55.065598\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 51.720314accuracy_test: 0.8757loss_test: 55.286415\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 61.830105accuracy_test: 0.8892loss_test: 55.181087\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 55.482227accuracy_test: 0.8897loss_test: 55.215687\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 55.283104accuracy_test: 0.8917loss_test: 54.84662\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 57.782345accuracy_test: 0.8927loss_test: 54.23585\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 59.29234accuracy_test: 0.891loss_test: 54.177048\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 57.922848accuracy_test: 0.8862loss_test: 54.305885\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 65.19651accuracy_test: 0.8835loss_test: 54.324276\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 49.068035accuracy_test: 0.8867loss_test: 53.663456\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 37.614532accuracy_test: 0.8885loss_test: 53.16552\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 49.144745accuracy_test: 0.8878loss_test: 53.18261\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 56.883247accuracy_test: 0.8839loss_test: 53.58125\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 42.894024accuracy_test: 0.8761loss_test: 54.348682\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 55.998158accuracy_test: 0.8693loss_test: 54.78964\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 44.379463accuracy_test: 0.8682loss_test: 54.617817\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 51.8985accuracy_test: 0.8698loss_test: 54.06397\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 58.7007accuracy_test: 0.877loss_test: 53.0708\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 47.200455accuracy_test: 0.8829loss_test: 52.338337\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 54.630314accuracy_test: 0.8859loss_test: 52.062023\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 31.066496accuracy_test: 0.8876loss_test: 52.12937\n",
      "\n",
      "\n",
      "accuracy_train: 0.81loss_train: 69.00575accuracy_test: 0.8834loss_test: 52.955215\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 35.109505accuracy_test: 0.8828loss_test: 53.269638\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 48.1497accuracy_test: 0.8889loss_test: 52.306408\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 63.389214accuracy_test: 0.8954loss_test: 50.85432\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 44.95932accuracy_test: 0.9008loss_test: 49.68963\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 44.609173accuracy_test: 0.9024loss_test: 49.238277\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 35.612717accuracy_test: 0.9023loss_test: 49.16122\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 46.470215accuracy_test: 0.9023loss_test: 48.978348\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 53.186573accuracy_test: 0.902loss_test: 49.143646\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 48.735477accuracy_test: 0.899loss_test: 49.5862\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 54.094826accuracy_test: 0.8964loss_test: 49.868362\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 36.88934accuracy_test: 0.8923loss_test: 50.186604\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 55.364845accuracy_test: 0.8964loss_test: 49.52257\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 45.73143accuracy_test: 0.8967loss_test: 49.08348\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 57.877953accuracy_test: 0.8986loss_test: 48.911983\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 55.23047accuracy_test: 0.9006loss_test: 48.64288\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 51.518875accuracy_test: 0.9019loss_test: 48.03837\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 50.308887accuracy_test: 0.9042loss_test: 47.436073\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 43.73025accuracy_test: 0.9018loss_test: 47.523716\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 40.03419accuracy_test: 0.901loss_test: 47.53781\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 38.778656accuracy_test: 0.9008loss_test: 47.481895\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 61.384594accuracy_test: 0.8958loss_test: 48.113632\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 48.307125accuracy_test: 0.8914loss_test: 49.14958\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 46.06591accuracy_test: 0.8921loss_test: 49.289974\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 37.97933accuracy_test: 0.894loss_test: 48.794685\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 61.421352accuracy_test: 0.9026loss_test: 47.57495\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 53.790386accuracy_test: 0.9003loss_test: 46.992718\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 48.74298accuracy_test: 0.8919loss_test: 47.975117\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 42.813698accuracy_test: 0.8729loss_test: 50.142525\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 39.563133accuracy_test: 0.8639loss_test: 51.240723\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 43.545414accuracy_test: 0.8706loss_test: 49.86564\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 38.633366accuracy_test: 0.8879loss_test: 47.55632\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 43.057728accuracy_test: 0.8969loss_test: 46.67047\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 43.55183accuracy_test: 0.8991loss_test: 47.072784\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 53.16976accuracy_test: 0.8948loss_test: 48.17071\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 58.254707accuracy_test: 0.8951loss_test: 48.246914\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 48.09014accuracy_test: 0.8995loss_test: 46.890045\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 36.670456accuracy_test: 0.8994loss_test: 46.177963\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 57.17006accuracy_test: 0.8983loss_test: 46.541416\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 44.118683accuracy_test: 0.8899loss_test: 47.977413\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 56.4447accuracy_test: 0.8833loss_test: 49.05401\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 56.534378accuracy_test: 0.8908loss_test: 47.824524\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 41.04969accuracy_test: 0.901loss_test: 45.829983\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 39.94804accuracy_test: 0.9044loss_test: 45.04717\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 40.048565accuracy_test: 0.9035loss_test: 45.416702\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 40.07332accuracy_test: 0.9024loss_test: 45.654835\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 49.84674accuracy_test: 0.9013loss_test: 45.832832\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 51.71907accuracy_test: 0.9031loss_test: 45.318474\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 60.145157accuracy_test: 0.906loss_test: 44.596447\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 56.42095accuracy_test: 0.906loss_test: 44.612614\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 29.760489accuracy_test: 0.9044loss_test: 45.116974\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 41.083687accuracy_test: 0.9043loss_test: 45.01529\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 55.63505accuracy_test: 0.9069loss_test: 44.21247\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 39.37994accuracy_test: 0.908loss_test: 43.598278\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 36.10056accuracy_test: 0.9101loss_test: 43.145714\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 32.39753accuracy_test: 0.9092loss_test: 43.310703\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 35.222046accuracy_test: 0.9071loss_test: 43.91671\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 41.97216accuracy_test: 0.9056loss_test: 44.13622\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 49.04878accuracy_test: 0.9078loss_test: 43.65603\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 53.31597accuracy_test: 0.9084loss_test: 43.395267\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 28.632936accuracy_test: 0.9116loss_test: 42.864494\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 51.068924accuracy_test: 0.9124loss_test: 42.558186\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 43.90455accuracy_test: 0.911loss_test: 42.57763\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 36.21826accuracy_test: 0.9069loss_test: 43.69247\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 48.76069accuracy_test: 0.8986loss_test: 44.923946\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 39.00557accuracy_test: 0.8987loss_test: 44.64537\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 35.89952accuracy_test: 0.9043loss_test: 43.616962\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 56.33134accuracy_test: 0.9093loss_test: 42.35374\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 37.37697accuracy_test: 0.9116loss_test: 41.628418\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 29.032368accuracy_test: 0.9136loss_test: 41.26045\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 41.928688accuracy_test: 0.9142loss_test: 41.511044\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 37.17667accuracy_test: 0.912loss_test: 42.155743\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 31.141563accuracy_test: 0.9103loss_test: 42.56079\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 28.464073accuracy_test: 0.9109loss_test: 42.190613\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 34.661785accuracy_test: 0.9144loss_test: 41.27968\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 38.70923accuracy_test: 0.911loss_test: 41.648224\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 56.43258accuracy_test: 0.9068loss_test: 42.5006\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200] ,stddev = 0.1))\n",
    "b1 = tf.Variable(tf.zeros([200]))\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([200,100], stddev = 0.1))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([100,60], stddev = 0.1))\n",
    "b3 = tf.Variable(tf.zeros([60]))\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([60,30], stddev = 0.1))\n",
    "b4 = tf.Variable(tf.zeros([30]))\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([30,10], stddev = 0.1))\n",
    "b5 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "init = tf.global_variables_initializer()  \n",
    "\n",
    "#the model\n",
    "#using sigmoid on the five layers and softmax on the output layer\n",
    "XX = tf.reshape(X,[-1,784])\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1)+b1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2)+b2)\n",
    "Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3)+b3)\n",
    "Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4)+b4)\n",
    "Ylogits = tf.matmul(Y4,W5)+b5\n",
    "Y  = tf.nn.softmax(Ylogits)  \n",
    "\n",
    "Y_ = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "#loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "            \n",
    "#% of correct answers found in the batch\n",
    "is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "  \n",
    "#performing gradient descent\n",
    "optimizer = tf.train.AdamOptimizer(0.003)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# training step, learning rate = 0.003\n",
    "learning_rate = 0.003\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#executing the training \n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(400):\n",
    "    # load batch of images and correct answers\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    train_data={X: batch_X, Y_: batch_Y}\n",
    "\n",
    "    # train\n",
    "    sess.run(train_step, feed_dict = train_data)\n",
    "    \n",
    "    #succes\n",
    "    a_train, c_train = sess.run([accuracy, cross_entropy], feed_dict=train_data)\n",
    "                \n",
    "   \n",
    "    # success on test data ?\n",
    "    test_data={X: mnist.test.images, Y_: mnist.test.labels}\n",
    "    a_test,c_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "    \n",
    "    print('accuracy_train: ' + str(a_train) + 'loss_train: ' + str(c_train) + 'accuracy_test: ' + str(a_test) + 'loss_test: ' + str(c_test))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1403,
     "status": "ok",
     "timestamp": 1539948934408,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "EvAe3QrIG95C",
    "outputId": "0243c4ce-7dd7-4ed0-bad0-e5abaef08f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is:  90.68%\n"
     ]
    }
   ],
   "source": [
    "a_test, c_test  = a_test,c_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "\n",
    "print('Test accuracy is: ', (\"%.2f\"%(a_test*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mirXr0y1P5m"
   },
   "source": [
    "# **with the hyperbolic tangent activation function, one hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16017
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26149,
     "status": "ok",
     "timestamp": 1539949171696,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "VZM3AdWR1HjL",
    "outputId": "31549553-cbb1-4ebc-96d5-7a7c46bfcfe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_train: 0.45loss_train: 185.72978accuracy_test: 0.3137loss_test: 21058.18\n",
      "\n",
      "\n",
      "accuracy_train: 0.52loss_train: 169.95116accuracy_test: 0.4256loss_test: 18712.164\n",
      "\n",
      "\n",
      "accuracy_train: 0.63loss_train: 152.18584accuracy_test: 0.4949loss_test: 17138.979\n",
      "\n",
      "\n",
      "accuracy_train: 0.72loss_train: 125.45307accuracy_test: 0.5959loss_test: 15203.088\n",
      "\n",
      "\n",
      "accuracy_train: 0.62loss_train: 130.18251accuracy_test: 0.6417loss_test: 13749.173\n",
      "\n",
      "\n",
      "accuracy_train: 0.74loss_train: 105.629906accuracy_test: 0.6628loss_test: 12648.857\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 106.92197accuracy_test: 0.6981loss_test: 11626.287\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 92.27069accuracy_test: 0.7102loss_test: 10993.062\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 84.172104accuracy_test: 0.7194loss_test: 10265.447\n",
      "\n",
      "\n",
      "accuracy_train: 0.76loss_train: 92.910225accuracy_test: 0.7729loss_test: 9297.733\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 72.223236accuracy_test: 0.7683loss_test: 9048.428\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 66.605316accuracy_test: 0.7811loss_test: 8473.275\n",
      "\n",
      "\n",
      "accuracy_train: 0.84loss_train: 65.479485accuracy_test: 0.7471loss_test: 8474.59\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 64.304535accuracy_test: 0.7895loss_test: 7948.508\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 64.52997accuracy_test: 0.7929loss_test: 7790.7993\n",
      "\n",
      "\n",
      "accuracy_train: 0.82loss_train: 69.47912accuracy_test: 0.8204loss_test: 7163.7734\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 52.32203accuracy_test: 0.812loss_test: 7068.0635\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 59.620148accuracy_test: 0.817loss_test: 6909.727\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 53.84336accuracy_test: 0.8164loss_test: 6863.4814\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 54.929886accuracy_test: 0.8284loss_test: 6454.7344\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 54.309597accuracy_test: 0.8381loss_test: 6197.833\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 53.79428accuracy_test: 0.8374loss_test: 6188.922\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 51.818867accuracy_test: 0.8466loss_test: 5975.292\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 51.751587accuracy_test: 0.8542loss_test: 5751.1826\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 52.0082accuracy_test: 0.8524loss_test: 5631.1006\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 53.42683accuracy_test: 0.8432loss_test: 5798.792\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 44.01827accuracy_test: 0.8364loss_test: 5733.9688\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 43.418415accuracy_test: 0.8477loss_test: 5534.254\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 42.165993accuracy_test: 0.8641loss_test: 5217.9307\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 41.18203accuracy_test: 0.8599loss_test: 5249.5703\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 42.91158accuracy_test: 0.8595loss_test: 5106.28\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 33.10624accuracy_test: 0.8528loss_test: 5195.3716\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 49.185913accuracy_test: 0.8632loss_test: 5006.5146\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 34.38656accuracy_test: 0.8598loss_test: 5056.119\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 34.33933accuracy_test: 0.865loss_test: 4912.313\n",
      "\n",
      "\n",
      "accuracy_train: 0.85loss_train: 49.784054accuracy_test: 0.8597loss_test: 4941.7275\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 34.30772accuracy_test: 0.8719loss_test: 4753.6494\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 31.993492accuracy_test: 0.874loss_test: 4721.7\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 42.77288accuracy_test: 0.8796loss_test: 4500.826\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 40.91307accuracy_test: 0.8769loss_test: 4521.528\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 46.334873accuracy_test: 0.8791loss_test: 4619.5176\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 36.285732accuracy_test: 0.8725loss_test: 4542.211\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 33.931602accuracy_test: 0.8754loss_test: 4563.7764\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 33.769787accuracy_test: 0.8628loss_test: 4695.053\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 37.324043accuracy_test: 0.8825loss_test: 4274.8994\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 39.12709accuracy_test: 0.873loss_test: 4485.114\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 25.161516accuracy_test: 0.8727loss_test: 4371.7812\n",
      "\n",
      "\n",
      "accuracy_train: 0.83loss_train: 50.31182accuracy_test: 0.8857loss_test: 4235.6675\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 28.060177accuracy_test: 0.8766loss_test: 4298.88\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 47.601585accuracy_test: 0.8661loss_test: 4525.2627\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 27.827215accuracy_test: 0.8719loss_test: 4320.417\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 34.580147accuracy_test: 0.846loss_test: 4668.4404\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 31.591986accuracy_test: 0.8795loss_test: 4221.8247\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 31.573896accuracy_test: 0.8749loss_test: 4161.1777\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 22.140848accuracy_test: 0.8867loss_test: 4013.4014\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 43.299038accuracy_test: 0.8729loss_test: 4426.28\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 30.972294accuracy_test: 0.8901loss_test: 3958.4863\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 27.610828accuracy_test: 0.8887loss_test: 3962.085\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 27.280561accuracy_test: 0.8872loss_test: 3987.1528\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 44.633816accuracy_test: 0.8871loss_test: 3995.0793\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 33.495163accuracy_test: 0.8901loss_test: 3897.4248\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 39.255146accuracy_test: 0.8871loss_test: 3916.552\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 31.520535accuracy_test: 0.8887loss_test: 3871.2485\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 28.228865accuracy_test: 0.882loss_test: 3998.4712\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 31.238468accuracy_test: 0.8657loss_test: 4369.2056\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 34.43291accuracy_test: 0.887loss_test: 3936.508\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 35.901577accuracy_test: 0.8894loss_test: 3829.0662\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 34.51272accuracy_test: 0.8761loss_test: 4134.2026\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 29.46994accuracy_test: 0.8803loss_test: 4027.2861\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 31.51698accuracy_test: 0.8918loss_test: 3811.333\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 24.257084accuracy_test: 0.8696loss_test: 4190.9307\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 24.893694accuracy_test: 0.8941loss_test: 3854.7476\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 34.08496accuracy_test: 0.8958loss_test: 3660.4155\n",
      "\n",
      "\n",
      "accuracy_train: 0.86loss_train: 39.425476accuracy_test: 0.8798loss_test: 4027.2905\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 22.308298accuracy_test: 0.8966loss_test: 3679.1335\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 32.823673accuracy_test: 0.8958loss_test: 3675.7144\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 34.19031accuracy_test: 0.8935loss_test: 3707.3335\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 27.929726accuracy_test: 0.8958loss_test: 3616.9622\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 30.833284accuracy_test: 0.8959loss_test: 3769.0298\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 29.721437accuracy_test: 0.8932loss_test: 3659.1082\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 36.46821accuracy_test: 0.8941loss_test: 3564.3208\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 37.172455accuracy_test: 0.8933loss_test: 3708.3345\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 28.562185accuracy_test: 0.8977loss_test: 3535.4543\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 22.947422accuracy_test: 0.9006loss_test: 3461.543\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 22.709793accuracy_test: 0.893loss_test: 3584.8066\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 23.012493accuracy_test: 0.8893loss_test: 3756.102\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 30.109726accuracy_test: 0.8956loss_test: 3573.6528\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.684124accuracy_test: 0.8992loss_test: 3450.2393\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 31.641853accuracy_test: 0.9014loss_test: 3463.0452\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 24.449026accuracy_test: 0.8915loss_test: 3744.9324\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 20.934822accuracy_test: 0.9019loss_test: 3450.646\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 19.63332accuracy_test: 0.9069loss_test: 3337.1748\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 32.185997accuracy_test: 0.8824loss_test: 3868.035\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 26.556953accuracy_test: 0.8897loss_test: 3709.1084\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 34.1833accuracy_test: 0.8929loss_test: 3590.2715\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 20.306787accuracy_test: 0.8952loss_test: 3530.9426\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 31.017056accuracy_test: 0.8993loss_test: 3420.6895\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 27.940935accuracy_test: 0.9041loss_test: 3384.0725\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 23.426683accuracy_test: 0.908loss_test: 3251.2576\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 20.536745accuracy_test: 0.9021loss_test: 3321.6604\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 20.929846accuracy_test: 0.9037loss_test: 3334.4917\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 27.563782accuracy_test: 0.9021loss_test: 3417.753\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 22.91108accuracy_test: 0.8993loss_test: 3519.8604\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 14.381474accuracy_test: 0.8999loss_test: 3402.25\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 29.805773accuracy_test: 0.8946loss_test: 3611.767\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 15.412953accuracy_test: 0.904loss_test: 3371.8643\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 14.150942accuracy_test: 0.9047loss_test: 3249.135\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 24.312347accuracy_test: 0.903loss_test: 3320.6086\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 20.903261accuracy_test: 0.9032loss_test: 3373.9553\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 19.283932accuracy_test: 0.9107loss_test: 3147.6917\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 15.218809accuracy_test: 0.9018loss_test: 3342.849\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 16.222712accuracy_test: 0.9061loss_test: 3325.963\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 25.530436accuracy_test: 0.9098loss_test: 3208.6167\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 23.908983accuracy_test: 0.9112loss_test: 3131.1768\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 23.633389accuracy_test: 0.9043loss_test: 3314.5032\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.144053accuracy_test: 0.908loss_test: 3186.9546\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 25.845402accuracy_test: 0.9061loss_test: 3265.0542\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 19.175255accuracy_test: 0.9088loss_test: 3156.665\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.493149accuracy_test: 0.9106loss_test: 3100.3494\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.106407accuracy_test: 0.9111loss_test: 3155.1465\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 13.143165accuracy_test: 0.9042loss_test: 3267.8066\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 22.466515accuracy_test: 0.9089loss_test: 3166.505\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 16.764523accuracy_test: 0.9037loss_test: 3258.8872\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 24.398378accuracy_test: 0.9108loss_test: 3119.3547\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 18.17052accuracy_test: 0.9086loss_test: 3153.9275\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 25.050524accuracy_test: 0.9072loss_test: 3239.8853\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.654005accuracy_test: 0.907loss_test: 3268.4302\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 12.82209accuracy_test: 0.9108loss_test: 3091.3804\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 14.82897accuracy_test: 0.8947loss_test: 3458.4412\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 21.981789accuracy_test: 0.9097loss_test: 3127.2664\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 33.325996accuracy_test: 0.9098loss_test: 3057.231\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 18.126114accuracy_test: 0.9137loss_test: 2989.945\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 26.236458accuracy_test: 0.9113loss_test: 3034.843\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 18.978424accuracy_test: 0.91loss_test: 3013.775\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 17.556074accuracy_test: 0.9097loss_test: 2999.7566\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 26.350897accuracy_test: 0.9057loss_test: 3165.0186\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 21.382713accuracy_test: 0.9088loss_test: 3068.3628\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.671333accuracy_test: 0.9026loss_test: 3248.0532\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 25.609255accuracy_test: 0.9088loss_test: 3018.783\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 18.091024accuracy_test: 0.907loss_test: 3146.2593\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 17.864513accuracy_test: 0.9118loss_test: 3046.041\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 40.55568accuracy_test: 0.9096loss_test: 3161.9758\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 13.810982accuracy_test: 0.912loss_test: 2955.0286\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 30.200394accuracy_test: 0.9082loss_test: 3125.3682\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 25.190645accuracy_test: 0.9052loss_test: 3115.8174\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 14.57494accuracy_test: 0.9143loss_test: 2949.683\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 23.449112accuracy_test: 0.9162loss_test: 2894.1897\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 13.647285accuracy_test: 0.914loss_test: 2953.772\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 18.749405accuracy_test: 0.9121loss_test: 3031.901\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 16.708302accuracy_test: 0.9039loss_test: 3363.2166\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 11.198194accuracy_test: 0.9172loss_test: 2943.2314\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 24.42793accuracy_test: 0.9088loss_test: 3113.1719\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 19.702581accuracy_test: 0.9139loss_test: 3016.6934\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 12.656085accuracy_test: 0.9145loss_test: 2988.795\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 13.165659accuracy_test: 0.9131loss_test: 3003.9888\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 16.468203accuracy_test: 0.9156loss_test: 2974.9128\n",
      "\n",
      "\n",
      "accuracy_train: 0.89loss_train: 41.121117accuracy_test: 0.9169loss_test: 2847.8516\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 23.502316accuracy_test: 0.9164loss_test: 2891.145\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.70472accuracy_test: 0.918loss_test: 2869.517\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.28956accuracy_test: 0.9118loss_test: 3009.7114\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 15.12839accuracy_test: 0.9112loss_test: 3009.9653\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 19.447748accuracy_test: 0.9152loss_test: 2952.2378\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 15.98261accuracy_test: 0.916loss_test: 2946.5151\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 25.398836accuracy_test: 0.9155loss_test: 2872.157\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 10.163855accuracy_test: 0.9145loss_test: 2909.9697\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 24.274574accuracy_test: 0.9177loss_test: 2904.2495\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 19.985792accuracy_test: 0.9116loss_test: 3059.709\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 18.0282accuracy_test: 0.9194loss_test: 2856.5884\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.881138accuracy_test: 0.9159loss_test: 2868.3735\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 39.464035accuracy_test: 0.9157loss_test: 2914.7825\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 26.467299accuracy_test: 0.9091loss_test: 3195.1252\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 18.23724accuracy_test: 0.9146loss_test: 2857.2957\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 20.073515accuracy_test: 0.9131loss_test: 3031.9329\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 23.415707accuracy_test: 0.9136loss_test: 2931.4082\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.801136accuracy_test: 0.9204loss_test: 2774.375\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 15.077704accuracy_test: 0.9224loss_test: 2770.4355\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 22.751904accuracy_test: 0.9175loss_test: 2869.3047\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 19.062641accuracy_test: 0.908loss_test: 3068.9492\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 27.908283accuracy_test: 0.909loss_test: 3037.3103\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 14.528125accuracy_test: 0.9088loss_test: 3098.5564\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 15.879778accuracy_test: 0.9176loss_test: 2847.5251\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 23.048742accuracy_test: 0.9183loss_test: 2778.3745\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 35.23502accuracy_test: 0.9154loss_test: 2922.1206\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 23.892746accuracy_test: 0.9167loss_test: 2821.643\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 19.929169accuracy_test: 0.9185loss_test: 2845.6147\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 20.99813accuracy_test: 0.92loss_test: 2815.394\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.233662accuracy_test: 0.9175loss_test: 2890.5864\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 30.558067accuracy_test: 0.9123loss_test: 3094.9094\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 29.622757accuracy_test: 0.8962loss_test: 3338.87\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 25.327444accuracy_test: 0.921loss_test: 2822.1313\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 31.207361accuracy_test: 0.9168loss_test: 2866.6323\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 20.983227accuracy_test: 0.9206loss_test: 2735.5596\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 22.190098accuracy_test: 0.9205loss_test: 2877.7212\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 18.618004accuracy_test: 0.9181loss_test: 2906.0757\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 16.573706accuracy_test: 0.9231loss_test: 2614.129\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 16.761646accuracy_test: 0.9142loss_test: 2869.1387\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.530336accuracy_test: 0.9174loss_test: 2818.645\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 19.502762accuracy_test: 0.9197loss_test: 2758.523\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 16.4308accuracy_test: 0.919loss_test: 2753.6067\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 19.262154accuracy_test: 0.9246loss_test: 2625.6895\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 9.917527accuracy_test: 0.926loss_test: 2584.5796\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 29.614166accuracy_test: 0.9254loss_test: 2645.2595\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 18.66143accuracy_test: 0.9235loss_test: 2655.6843\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 27.86889accuracy_test: 0.9104loss_test: 2979.9502\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 13.984047accuracy_test: 0.9186loss_test: 2750.7217\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 13.071047accuracy_test: 0.9162loss_test: 2830.4734\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 22.296425accuracy_test: 0.9115loss_test: 3017.707\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.41366accuracy_test: 0.9211loss_test: 2767.8462\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 12.323551accuracy_test: 0.9212loss_test: 2735.1016\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 17.644016accuracy_test: 0.9172loss_test: 2824.6323\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 17.51547accuracy_test: 0.9193loss_test: 2749.0015\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 12.492155accuracy_test: 0.9206loss_test: 2737.6284\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 16.345993accuracy_test: 0.9235loss_test: 2705.1711\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 17.467445accuracy_test: 0.9157loss_test: 2845.0525\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 22.312252accuracy_test: 0.9175loss_test: 2784.9585\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 17.86552accuracy_test: 0.9077loss_test: 3049.8079\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 16.875032accuracy_test: 0.9276loss_test: 2567.5059\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 11.999201accuracy_test: 0.9206loss_test: 2674.7124\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 20.143364accuracy_test: 0.9282loss_test: 2552.808\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 21.167366accuracy_test: 0.9248loss_test: 2579.7092\n",
      "\n",
      "\n",
      "accuracy_train: 1.0loss_train: 10.106933accuracy_test: 0.9249loss_test: 2578.976\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 13.672953accuracy_test: 0.923loss_test: 2638.5337\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 23.200916accuracy_test: 0.926loss_test: 2549.6494\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 19.53395accuracy_test: 0.8982loss_test: 3241.1162\n",
      "\n",
      "\n",
      "accuracy_train: 0.9loss_train: 26.246056accuracy_test: 0.9104loss_test: 2971.8774\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 14.369011accuracy_test: 0.9262loss_test: 2498.782\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 17.243097accuracy_test: 0.9261loss_test: 2626.3376\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 17.226475accuracy_test: 0.9235loss_test: 2655.9575\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 27.020145accuracy_test: 0.9195loss_test: 2715.2793\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 17.036324accuracy_test: 0.9217loss_test: 2673.2769\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 20.476828accuracy_test: 0.9201loss_test: 2669.9778\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 23.778915accuracy_test: 0.9197loss_test: 2658.3513\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 19.746176accuracy_test: 0.9272loss_test: 2465.5894\n",
      "\n",
      "\n",
      "accuracy_train: 0.92loss_train: 24.390884accuracy_test: 0.9291loss_test: 2505.7559\n",
      "\n",
      "\n",
      "accuracy_train: 1.0loss_train: 9.581731accuracy_test: 0.9165loss_test: 2725.8228\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 9.886437accuracy_test: 0.9278loss_test: 2516.5498\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 16.416035accuracy_test: 0.9249loss_test: 2558.9436\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 22.301151accuracy_test: 0.9263loss_test: 2542.8208\n",
      "\n",
      "\n",
      "accuracy_train: 0.88loss_train: 30.28035accuracy_test: 0.9228loss_test: 2609.2612\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 18.603928accuracy_test: 0.9275loss_test: 2497.8967\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 12.263786accuracy_test: 0.9297loss_test: 2446.9136\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 12.808344accuracy_test: 0.9266loss_test: 2519.1882\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 15.597967accuracy_test: 0.9265loss_test: 2513.2102\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 27.070393accuracy_test: 0.9296loss_test: 2458.607\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 19.29594accuracy_test: 0.9222loss_test: 2637.301\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 21.092066accuracy_test: 0.9292loss_test: 2452.6436\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.5506accuracy_test: 0.9266loss_test: 2455.396\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 22.116982accuracy_test: 0.9246loss_test: 2516.7314\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 14.2445345accuracy_test: 0.9287loss_test: 2448.805\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 20.501316accuracy_test: 0.9236loss_test: 2617.4587\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 33.163986accuracy_test: 0.928loss_test: 2467.109\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 10.746804accuracy_test: 0.9281loss_test: 2446.776\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 16.142029accuracy_test: 0.9313loss_test: 2416.5776\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 9.638192accuracy_test: 0.9316loss_test: 2366.488\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 23.851368accuracy_test: 0.921loss_test: 2691.3755\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 14.558769accuracy_test: 0.9246loss_test: 2628.2856\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 17.869747accuracy_test: 0.9248loss_test: 2508.0447\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.414602accuracy_test: 0.9205loss_test: 2613.5293\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 19.009544accuracy_test: 0.9249loss_test: 2534.1006\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 14.680174accuracy_test: 0.926loss_test: 2455.7195\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 19.337463accuracy_test: 0.9279loss_test: 2509.4307\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 16.525108accuracy_test: 0.9185loss_test: 2686.8794\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 24.043974accuracy_test: 0.9263loss_test: 2496.938\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 16.126644accuracy_test: 0.9324loss_test: 2355.8691\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 13.655478accuracy_test: 0.9293loss_test: 2446.6052\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 21.148685accuracy_test: 0.9282loss_test: 2413.377\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 12.600796accuracy_test: 0.9275loss_test: 2455.1855\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 24.646374accuracy_test: 0.9303loss_test: 2360.5369\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 13.358073accuracy_test: 0.9309loss_test: 2336.1846\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 18.637182accuracy_test: 0.9309loss_test: 2345.4744\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 18.744576accuracy_test: 0.9303loss_test: 2334.7852\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.669811accuracy_test: 0.9286loss_test: 2427.5544\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.5864accuracy_test: 0.9229loss_test: 2560.4429\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 22.219868accuracy_test: 0.9268loss_test: 2507.978\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 16.21403accuracy_test: 0.9271loss_test: 2461.3857\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 13.297394accuracy_test: 0.9293loss_test: 2405.5493\n",
      "\n",
      "\n",
      "accuracy_train: 1.0loss_train: 5.502364accuracy_test: 0.9289loss_test: 2368.2913\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 18.161734accuracy_test: 0.9272loss_test: 2424.7544\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 20.058475accuracy_test: 0.9176loss_test: 2701.3853\n",
      "\n",
      "\n",
      "accuracy_train: 0.94loss_train: 20.575708accuracy_test: 0.9272loss_test: 2428.3777\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 15.586725accuracy_test: 0.928loss_test: 2406.9983\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 9.219058accuracy_test: 0.9278loss_test: 2423.269\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 15.777296accuracy_test: 0.931loss_test: 2314.4917\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 14.181015accuracy_test: 0.9264loss_test: 2415.9727\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 12.655141accuracy_test: 0.9298loss_test: 2379.532\n",
      "\n",
      "\n",
      "accuracy_train: 0.95loss_train: 16.30264accuracy_test: 0.9312loss_test: 2314.031\n",
      "\n",
      "\n",
      "accuracy_train: 0.87loss_train: 24.287075accuracy_test: 0.9307loss_test: 2306.0088\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 13.869379accuracy_test: 0.9293loss_test: 2404.2627\n",
      "\n",
      "\n",
      "accuracy_train: 1.0loss_train: 8.065502accuracy_test: 0.933loss_test: 2328.2788\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 14.615652accuracy_test: 0.9308loss_test: 2353.7588\n",
      "\n",
      "\n",
      "accuracy_train: 0.91loss_train: 20.847586accuracy_test: 0.9287loss_test: 2372.0237\n",
      "\n",
      "\n",
      "accuracy_train: 0.93loss_train: 23.36189accuracy_test: 0.9202loss_test: 2653.8994\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 14.546757accuracy_test: 0.9287loss_test: 2417.7449\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 14.114823accuracy_test: 0.9328loss_test: 2312.174\n",
      "\n",
      "\n",
      "accuracy_train: 0.99loss_train: 9.855749accuracy_test: 0.9325loss_test: 2286.221\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 13.06511accuracy_test: 0.9244loss_test: 2497.773\n",
      "\n",
      "\n",
      "accuracy_train: 0.97loss_train: 18.589506accuracy_test: 0.93loss_test: 2345.7915\n",
      "\n",
      "\n",
      "accuracy_train: 0.98loss_train: 9.118225accuracy_test: 0.9357loss_test: 2226.3057\n",
      "\n",
      "\n",
      "accuracy_train: 1.0loss_train: 7.510645accuracy_test: 0.9326loss_test: 2254.934\n",
      "\n",
      "\n",
      "accuracy_train: 0.96loss_train: 17.18009accuracy_test: 0.9307loss_test: 2296.7498\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_DATA\", one_hot = True)\n",
    "\n",
    "'''\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([28*28, 200] ,stddev = 0.1))\n",
    "b1 = tf.Variable(tf.zeros([200]))\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([200,100], stddev = 0.1))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([100,10], stddev = 0.1))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "init = tf.initialize_all_variables()           \n",
    "\n",
    "#the model\n",
    "#using tanh on the first & second layers and softmax onthe output layer\n",
    "XX = tf.reshape(X,[-1,784])\n",
    "Y1 = tf.nn.tanh(tf.matmul(XX, W1)+b1)\n",
    "Y2 = tf.nn.tanh(tf.matmul(Y1, W2)+b2)\n",
    "Y  = tf.nn.softmax(tf.matmul(Y2, W3)+b3)  \n",
    "\n",
    "Y_ = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "#loss function\n",
    "cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y))  \n",
    "            \n",
    "#% of correct answers found in the batch\n",
    "is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "  \n",
    "#performing gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.003)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "                \n",
    "#executing the training \n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(300):\n",
    "    # load batch of images and correct answers\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    train_data={X: batch_X, Y_: batch_Y}\n",
    "\n",
    "    # train\n",
    "    sess.run(train_step, feed_dict = train_data)\n",
    "    \n",
    "    #succes\n",
    "    a,c = sess.run([accuracy, cross_entropy], feed_dict=train_data)\n",
    "                \n",
    "   \n",
    "    # success on test data ?\n",
    "    test_data={X: mnist.test.images, Y_: mnist.test.labels}\n",
    "    a_t,c_t = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "    \n",
    "    print('accuracy_train: ' + str(a) + 'loss_train: ' + str(c) + 'accuracy_test: ' + str(a_t) + 'loss_test: ' + str(c_t))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1767,
     "status": "ok",
     "timestamp": 1539949180208,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "t2g7TUHHHQjh",
    "outputId": "5caecbaf-dd31-46ba-a725-1bc8ac37b793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is:  93.07%\n"
     ]
    }
   ],
   "source": [
    "a_test, c_test  = a_test,c_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "\n",
    "print('Test accuracy is: ', (\"%.2f\"%(a_test*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXg0g-Ud1T0m"
   },
   "source": [
    "# Using Keras on tensorflow with relu\n",
    "with two hidden dense layers of 128 neurons each and relu activation function and an\n",
    "Output layer of 10 neurons softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7234,
     "status": "ok",
     "timestamp": 1539985994658,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "SjDD9GflATtS",
    "outputId": "46db0b00-2b33-4765-adaa-7546f111a856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3501,
     "status": "ok",
     "timestamp": 1539986002164,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "TqjCSyhUAU4L",
    "outputId": "605b3a49-8357-423e-ad60-b56df63edc0d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nABgQlgBg8H9+m3ORT6ubggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f38c44967b8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the image on train[54999]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[54999], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3501
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2277,
     "status": "ok",
     "timestamp": 1539986821862,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "Wq3ZRMCsaJkU",
    "outputId": "aa655568-45b7-4efe-8c6d-5448f6548fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.51669853e-04 4.44495021e-04\n",
      "  5.32451986e-04 1.20891592e-03 4.90527808e-04 9.77148455e-04\n",
      "  7.38207186e-04 8.25361008e-04 5.45230335e-04 9.66117352e-04\n",
      "  4.96647460e-04 9.33860076e-05 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.11909406e-04 1.31779700e-03\n",
      "  1.32849406e-03 1.35398583e-03 1.44306436e-03 1.51068350e-03\n",
      "  1.58998471e-03 1.59993057e-03 1.61644758e-03 1.49955638e-03\n",
      "  1.47241365e-03 1.02724608e-03 3.47535317e-04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 8.37300283e-04 1.27088877e-03 1.31779700e-03\n",
      "  1.32849406e-03 1.35398583e-03 1.44306436e-03 1.51068350e-03\n",
      "  1.58998471e-03 1.59993057e-03 1.61644758e-03 1.49955638e-03\n",
      "  1.47241365e-03 1.23859336e-03 9.47823591e-04 8.76136715e-05\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.90883135e-04 1.24609983e-03 1.31795872e-03 1.31779700e-03\n",
      "  1.32849406e-03 1.35398583e-03 1.44306436e-03 1.51068350e-03\n",
      "  1.58998471e-03 1.09836503e-03 1.61644758e-03 1.49955638e-03\n",
      "  1.47241365e-03 1.23859336e-03 1.32695303e-03 7.54825478e-04\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  6.97652177e-04 1.25595042e-03 1.32318871e-03 1.32302636e-03\n",
      "  1.33376587e-03 1.35935879e-03 1.45447199e-03 1.28887680e-03\n",
      "  1.95593357e-04 0.00000000e+00 5.06743488e-04 3.73407320e-04\n",
      "  1.04003821e-03 1.24350842e-03 1.33221871e-03 1.17267530e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.47737450e-04\n",
      "  1.18749307e-03 1.24609983e-03 1.31795872e-03 1.31779700e-03\n",
      "  1.32849406e-03 1.35398583e-03 6.04604042e-04 1.07905964e-04\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.96647460e-04 1.23859336e-03 1.32695303e-03 1.69835732e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.84144924e-04\n",
      "  1.24686772e-03 1.24609983e-03 1.31795872e-03 1.31779700e-03\n",
      "  1.04381676e-03 2.95512781e-04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.11015315e-04 1.02724608e-03 1.32695303e-03 1.69835732e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.60293422e-04\n",
      "  1.24686772e-03 1.24609983e-03 1.01984901e-03 4.39265668e-04\n",
      "  1.47610452e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 9.68265447e-04 1.32695303e-03 1.69835732e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.60293422e-04\n",
      "  1.24686772e-03 1.24609983e-03 8.73409152e-04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.80459742e-04 1.11571704e-03 1.32695303e-03 1.69835732e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.20024707e-04 1.43759903e-03\n",
      "  1.25181561e-03 6.94466705e-04 7.84499238e-05 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  8.58907960e-04 1.24350842e-03 1.33221871e-03 1.28050751e-03\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.08989318e-04 1.29694223e-03 1.43191682e-03\n",
      "  1.00936911e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.33952467e-03\n",
      "  1.45488491e-03 1.23859336e-03 1.27429616e-03 4.98723976e-04\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 7.83575661e-04 1.41484607e-03 1.21599285e-03\n",
      "  4.30466237e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  8.20230206e-05 9.52339625e-04 1.26365148e-03 1.49955638e-03\n",
      "  1.47241365e-03 1.23859336e-03 6.00288275e-04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.84165988e-04 1.52696795e-03 1.41484607e-03 4.77305606e-04\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.65410540e-04 4.55602961e-04\n",
      "  1.17986960e-03 1.59993057e-03 1.61644758e-03 1.49955638e-03\n",
      "  1.41398453e-03 5.60316045e-04 1.47439225e-04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.60082743e-03 1.68770142e-03 1.41484607e-03 4.77305606e-04\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.67339302e-04\n",
      "  7.38052258e-04 7.52214352e-04 1.44306436e-03 1.51068350e-03\n",
      "  1.58998471e-03 1.59993057e-03 1.61644758e-03 1.12614906e-03\n",
      "  4.32375436e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.61499405e-03 1.69439865e-03 1.42046054e-03 1.43759903e-03\n",
      "  1.25181561e-03 1.25595042e-03 1.32318871e-03 1.32302636e-03\n",
      "  1.33376587e-03 1.35935879e-03 1.45447199e-03 1.51667828e-03\n",
      "  1.59629417e-03 1.52374340e-03 4.04111895e-04 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.60082743e-03 1.68770142e-03 1.41484607e-03 1.43191682e-03\n",
      "  1.24686772e-03 1.24609983e-03 1.31795872e-03 1.31779700e-03\n",
      "  1.32849406e-03 1.35398583e-03 1.44306436e-03 1.05508054e-03\n",
      "  8.58086985e-04 1.52374340e-04 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.70582336e-03 1.68770142e-03 1.41484607e-03 1.43191682e-03\n",
      "  1.24686772e-03 1.24609983e-03 1.05122898e-03 1.01972387e-03\n",
      "  1.02800136e-03 6.34009239e-04 3.19413456e-04 3.59686548e-05\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  5.38331349e-04 8.70639623e-04 1.25202649e-03 8.23920392e-04\n",
      "  1.10337897e-03 4.13724846e-04 4.70699543e-05 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#normalizing the dataset\n",
    "#it helps the model to train well\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(x_train[54999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFsoS65zT382"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 293102,
     "status": "ok",
     "timestamp": 1539988307043,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "dPvle9cwZMFy",
    "outputId": "ac984a38-19d0-49d6-921f-3848dd4a9cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0813 - acc: 0.9775\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.0788 - acc: 0.9780\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 15s 242us/step - loss: 0.0772 - acc: 0.9780\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0751 - acc: 0.9786\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0734 - acc: 0.9794\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0715 - acc: 0.9802\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0696 - acc: 0.9804\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.0682 - acc: 0.9808\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0661 - acc: 0.9816\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0647 - acc: 0.9821\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0627 - acc: 0.9826\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 0.0618 - acc: 0.9829\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 15s 242us/step - loss: 0.0600 - acc: 0.9837\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0583 - acc: 0.9841\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0570 - acc: 0.9842\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0557 - acc: 0.9850\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.0545 - acc: 0.9850\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0533 - acc: 0.9853\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.0518 - acc: 0.9861\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0508 - acc: 0.9863\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1765,
     "status": "ok",
     "timestamp": 1539988315229,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "7XHBZTrgUpmG",
    "outputId": "3ff17830-fee8-41c4-fd0d-0942cba3a268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'accuracy')"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f38b1b10320>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EjCXxuR8U4qW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 728
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 380413,
     "status": "ok",
     "timestamp": 1539990114972,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "YV5CdXRqbpjL",
    "outputId": "24dfeb19-6c5e-47a4-a391-19d74ecb67c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 16s 258us/step - loss: 0.9963 - acc: 0.6841\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.5375 - acc: 0.8417\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 19s 321us/step - loss: 0.4257 - acc: 0.8741\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 18s 306us/step - loss: 0.3697 - acc: 0.8920\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 18s 303us/step - loss: 0.3294 - acc: 0.9047\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 19s 312us/step - loss: 0.2986 - acc: 0.9126\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.2754 - acc: 0.9194\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 19s 321us/step - loss: 0.2574 - acc: 0.9243\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 20s 336us/step - loss: 0.2414 - acc: 0.9285\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 20s 335us/step - loss: 0.2288 - acc: 0.9322\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 19s 311us/step - loss: 0.2154 - acc: 0.9364\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 19s 312us/step - loss: 0.2026 - acc: 0.9397\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 19s 316us/step - loss: 0.1917 - acc: 0.9433\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 19s 318us/step - loss: 0.1817 - acc: 0.9458\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 19s 314us/step - loss: 0.1718 - acc: 0.9491\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 19s 321us/step - loss: 0.1633 - acc: 0.9513\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 19s 321us/step - loss: 0.1551 - acc: 0.9524\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 19s 319us/step - loss: 0.1475 - acc: 0.9556\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 20s 326us/step - loss: 0.1403 - acc: 0.9577\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 19s 315us/step - loss: 0.1338 - acc: 0.9590\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1657,
     "status": "ok",
     "timestamp": 1539990136789,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "98Rbdaz09BpH",
    "outputId": "d4e61b53-a332-47da-d328-9658e7c447b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'accuracy')"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f38b142dac8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_YTiobvmGGG"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19EQ9n9u6s-G"
   },
   "source": [
    "# Using Keras on CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1370,
     "status": "ok",
     "timestamp": 1540006020134,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "D7OHBdsFP4M-",
    "outputId": "344f41d8-8ef9-47c7-9da4-fb8adeea5c0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 811,
     "status": "ok",
     "timestamp": 1540006027001,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "7wAfyVRDgkfy",
    "outputId": "c6f9d95c-d34c-43a7-a170-85a831f43b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_fg9dOtQTE6"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 184796,
     "status": "ok",
     "timestamp": 1539952222683,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "CPGkVzgGP_c5",
    "outputId": "e879d42a-3d88-4275-d83e-7733ef1f06e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 61s 1ms/step - loss: 0.1746 - acc: 0.9449\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0475 - acc: 0.9853\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0347 - acc: 0.9896\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1539952231995,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "h1r2seGbQ90t",
    "outputId": "532f7c8a-b436-4784-e51d-db1e9ff0599f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'accuracy & loss')"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c61cecd30>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy & loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4274,
     "status": "ok",
     "timestamp": 1539952245371,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "W_Tjm4X1Q_Rp",
    "outputId": "697f8164-2d29-4eb6-f341-e9656b8b230d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 353us/step\n",
      "Test accuracy is:  99.17%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy is: ', (\"%.2f\"%(test_acc*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1539951992755,
     "user": {
      "displayName": "chisom maryann",
      "photoUrl": "",
      "userId": "13486482862699622935"
     },
     "user_tz": -60
    },
    "id": "JYHkxCeNSlzp",
    "outputId": "23419e5a-98cf-4487-b862-16ca40aa5cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is:  99.11%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy is: ', (\"%.2f\"%(test_acc*100))+'%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MnistDigitRecogn.ipynb",
   "provenance": [
    {
     "file_id": "1HJray3bdfg2yrzdOBIGXlflZEyXj_wiM",
     "timestamp": 1539696347351
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
